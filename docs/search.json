[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Données des aires protégées à Madagascar",
    "section": "",
    "text": "1 Conserver l’histoire des aires protégées à Madagascar : un modèle de données dynamique (2000-2024)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Conserver l'histoire des aires protégées à Madagascar : un modèle de données dynamique (2000-2024)</span>"
    ]
  },
  {
    "objectID": "index.html#résumé",
    "href": "index.html#résumé",
    "title": "Données des aires protégées à Madagascar",
    "section": "1.1 Résumé",
    "text": "1.1 Résumé\nLes données spatiales sur les aires protégées sont essentielles pour les études de conservation. Cependant, les bases existantes se limitent souvent aux frontières actuelles, ignorant les évolutions historiques. Cette lacune engendre des biais dans l’analyse de leur impact environnemental et social. Dans ce projet, nous proposons un modèle de données dynamique documentant l’évolution des aires protégées à Madagascar de 2000 à 2024. Ce modèle inclut chaque état successif avec ses périodes de validité, fournissant une base analytique plus robuste. Les données seront publiées en accès libre pour favoriser leur utilisation par la communauté scientifique et les praticiens.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Conserver l'histoire des aires protégées à Madagascar : un modèle de données dynamique (2000-2024)</span>"
    ]
  },
  {
    "objectID": "index.html#contexte-et-enjeux",
    "href": "index.html#contexte-et-enjeux",
    "title": "Données des aires protégées à Madagascar",
    "section": "1.2 Contexte et enjeux",
    "text": "1.2 Contexte et enjeux\n\n1.2.1 Importance des données ouvertes\nMadagascar, confronté à une extrême pauvreté, dispose de ressources limitées pour gérer ses données administratives, notamment dans le domaine de la conservation. Par exemple, les données relatives à la conservation à Madagascar ont longtemps été rendues accessibles sur le portail “Redbioma”, mais ce dernier aujourd’hui hors ligne et les jeux de données qu’il contenait sont introuvables. La mise à disposition de données sur un portail open data pérenne adossé à des procédures d’archivage renforce leur accessibilité, transparence, reproductibilité, et surtout leur persistance.\n\n\n1.2.2 Limites des approches actuelles\nLes bases de données nationales et internationales incluent généralement les caractéristiques actuelles des aires protégées sans tenir compte des modifications passées. Cette approche statique entraîne des pertes d’informations et des biais dans l’évaluation des impacts ou l’étude des dynamiques historiques, notamment dans le contexte des changements de limites (“degazetting”).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Conserver l'histoire des aires protégées à Madagascar : un modèle de données dynamique (2000-2024)</span>"
    ]
  },
  {
    "objectID": "index.html#originalité-de-la-solution",
    "href": "index.html#originalité-de-la-solution",
    "title": "Données des aires protégées à Madagascar",
    "section": "1.3 Originalité de la solution",
    "text": "1.3 Originalité de la solution\nNotre méthodologie repose sur :\n\nUne documentation exhaustive : Compilation de données issues de cartographies historiques, décrets, gestionnaires d’aires protégées, et autres sources.\nTraçabilité et contrôle de version : Chaque modification est documentée dans un système de gestion de version pour garantir une transparence totale.\nFormats adaptés : Les données sont proposées au format GeoParquet pour les analyses standard et en GML ou OGC Moving Features pour des besoins avancés.\nConstruction rétrospective : La base est reconstruite à partir des états récents vers les états passés.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Conserver l'histoire des aires protégées à Madagascar : un modèle de données dynamique (2000-2024)</span>"
    ]
  },
  {
    "objectID": "index.html#méthodologie",
    "href": "index.html#méthodologie",
    "title": "Données des aires protégées à Madagascar",
    "section": "1.4 Méthodologie",
    "text": "1.4 Méthodologie\n\nHarmonisation des données SAPM : Consolidation des versions successives, notamment entre 2002 et 2011.\nHarmonisation des données WDPA : Intégration des données internationales selon la même approche.\nCréation d’une base réglementaire : Extraction et structuration des textes juridiques via la base CNLEGIS.\nAnalyse et consolidation : Croisement des sources pour produire un ensemble cohérent.\nOutil de vérification visuelle : Développement d’un outil interactif pour valider la qualité et la traçabilité des données consolidées.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Conserver l'histoire des aires protégées à Madagascar : un modèle de données dynamique (2000-2024)</span>"
    ]
  },
  {
    "objectID": "index.html#résultats-attendus",
    "href": "index.html#résultats-attendus",
    "title": "Données des aires protégées à Madagascar",
    "section": "1.5 Résultats attendus",
    "text": "1.5 Résultats attendus\nNous publions un jeu de données couvrant Madagascar entre 2000 et 2024, accessible via un portail pérenne. Ce modèle permettra de corriger des biais observés dans des études emblématiques, offrant des conclusions plus fiables sur l’impact de la conservation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Conserver l'histoire des aires protégées à Madagascar : un modèle de données dynamique (2000-2024)</span>"
    ]
  },
  {
    "objectID": "index.html#perspectives",
    "href": "index.html#perspectives",
    "title": "Données des aires protégées à Madagascar",
    "section": "1.6 Perspectives",
    "text": "1.6 Perspectives\nCe projet contribue à une meilleure compréhension des dynamiques de conservation et propose un cadre méthodologique réplicable dans d’autres contextes. En intégrant des pratiques ouvertes et transparentes, il répond aux défis de la gestion des données dans les pays en développement tout en offrant des outils adaptés aux besoins des chercheurs et gestionnaires.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Conserver l'histoire des aires protégées à Madagascar : un modèle de données dynamique (2000-2024)</span>"
    ]
  },
  {
    "objectID": "01_sapm_data.html",
    "href": "01_sapm_data.html",
    "title": "2  Harmonisation des données SAPM",
    "section": "",
    "text": "2.1 ANGAP 2002\nDans cette section, nous passons en revue plusieurs jeux de données ayant documenté les aires protégées à Madagascar à différentes périodes. Ces données, issues de sources officielles, ont été produites et mises à jour par différentes institutions. Nous les harmonisons en appliquant un ensemble de traitements standardisés :\nCes opérations visent à rendre comparables des jeux de données produits à des moments différents et selon des logiques institutionnelles variées. Elles permettent la construction d’une série temporelle cohérente, indispensable à toute analyse historique ou comparative sur les dynamiques de conservation à Madagascar.\nCes opérations sont réalisées avec R, avec une série de package appropriés.\nAvant la création du SAPM, l’ANGAP était la seule entité chargée de gérer les aires protégées, et donc leurs données. L’ANGAP avait dans ce cadre transmis à l’institut national de géographie malgache (FTM) pour la mise à jour de la base de donnée topographique (BD 500).\nCode\n# Load and preprocess the shapefile\nangap_2002 &lt;- st_read(\n  paste0(data_dir, \"ap_angap BD 500/ap_angap.shp\"),\n  quiet = TRUE\n) %&gt;%\n  st_set_crs(my_crs) %&gt;% # Laborde Magasiaraka CRS\n  clean_pa_names_cols(name_cols = \"NOM\") %&gt;%\n  mutate(\n    has_info = ifelse(\n      is.na(CLASSEMENT),\n      \"Attributs absents\",\n      \"Attributs présents\"\n    ),\n    has_name = case_when(\n      is.na(NOM) ~ \"Pas de nom\",\n      NOM == \"Hors AP\" ~ \"Hors AP\",\n      .default = \"Nommé\"\n    )\n  )\n\n# Separate datasets: \"Pas de nom\", \"Hors AP\", and \"Nommé\"\nangap_pas_de_nom &lt;- angap_2002 %&gt;% filter(has_name == \"Pas de nom\")\nangap_hors_ap &lt;- angap_2002 %&gt;% filter(has_name == \"Hors AP\")\nangap_named &lt;- angap_2002 %&gt;% filter(has_name == \"Nommé\")\n\n# Set tmap to interactive mode\ntmap_mode(\"view\")\n\n# Create the map with three layers\ntm_shape(angap_named) +\n  tm_fill(\n    col = \"has_info\",\n    id = \"NOM\",\n    popup.vars = names(st_drop_geometry(angap_named))\n  ) +\n  tm_borders(col = \"black\", lwd = 0.5) + # Default borders for named features\n  tm_shape(angap_pas_de_nom) +\n  tm_fill(\n    col = \"has_info\",\n    id = \"NOM\",\n    popup.vars = names(st_drop_geometry(angap_pas_de_nom)),\n    legend.show = FALSE\n  ) +\n  tm_borders(col = \"red\", lwd = 3) + # Thick red borders for \"Pas de nom\"\n  tm_shape(angap_hors_ap) +\n  tm_fill(\n    col = \"has_info\",\n    id = \"NOM\",\n    popup.vars = names(st_drop_geometry(angap_hors_ap)),\n    legend.show = FALSE\n  ) +\n  tm_borders(col = \"blue\", lwd = 3) + # Thick blue borders for \"Hors AP\"\n  tm_add_legend(\n    type = \"fill\",\n    labels = c(\"Pas de nom\", \"Hors AP\"),\n    col = c(\"red\", \"blue\"),\n    lwd = c(3, 3, 0.5),\n    title = \"Contour\"\n  )\nLe champ “classement” de ce jeu de données fait référence à des décrets parus jusqu’en 1998, mais il fait aussi référence à 11 entités qui n’ont pas de métadonnées les caractérisant.\nParmi celles-ci, 6 n’ont pas de nom :\nLe jeu de données n’inclut pas de métadonnées explicitant sa date, mais il nous a été remis avec l’explication qu’il s’agissait d’un jeu datant de 2002. Cette assertion est cohérente avec le fait que le jeu de donnée inclut l’extension d’Ankarafantsika opérée en 2002, mais pas les création [Compléter].\nOn effectue donc les opération suivantes : identification des polygones sans nom qui font visiblement partie d’une AP, regroupement des polygones simples en multipolygones s’ils ont les mêmes attributs, et suppression des polygones explicitement référencés comme “Hors AP”.\nCode\nangap_2002 &lt;- angap_2002 %&gt;%\n  select(APBON = APBON_, APBON_ID, NOM, TYPE_AP, CLASSEMENT, IUCN) %&gt;%\n  mutate(\n    NOM = case_when(\n      # Rename the patches visibly included in PAs\n      APBON_ID %in% c(100006, 100005) ~ \"Ranomafana\",\n      APBON_ID %in% c(100002, 100003, 100004) ~ \"Masoala\",\n      APBON == 27 ~ \"Baie de Baly\",\n      .default = NOM\n    )\n  ) %&gt;%\n  group_by(across(NOM:IUCN)) %&gt;%\n  summarize(geometry = st_union(geometry)) %&gt;%\n  ungroup() %&gt;%\n  filter(NOM != \"Hors AP\") # Discard the explicitely labelled as not PA\n\nwrite_rds(angap_2002, \"data/no_id/angap_2002.rds\")\n\ntm_shape(angap_2002) +\n  tm_polygons()\nLe résultat est présenté dans la carte ci-dessus.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Harmonisation des données SAPM</span>"
    ]
  },
  {
    "objectID": "01_sapm_data.html#angap-2002",
    "href": "01_sapm_data.html#angap-2002",
    "title": "2  Harmonisation des données SAPM",
    "section": "",
    "text": "Le Parc National d’Ankarafantsika, né en 2002 de la fusion de la réserve naturelle intégrale n°15 d’Ankarafantsika et de la réserve - forestière d’Anktarafantsika.\nMantadia, dont les limites ont été étendues en 2002.\nTsingy de Namoroka, créé en 1927 et devenu parc national en 2002.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Harmonisation des données SAPM</span>"
    ]
  },
  {
    "objectID": "01_sapm_data.html#sapm-2010",
    "href": "01_sapm_data.html#sapm-2010",
    "title": "2  Harmonisation des données SAPM",
    "section": "2.2 SAPM 2010",
    "text": "2.2 SAPM 2010\nLe site REBIOMA qui mettait en ligne les données du SAPM ne fonctionne plus depuis 2019 d’après les sauvegardes régulières effectuées par Internet Archive. Fort heureusement, une copie des données datées du 10/12/2010 a été sauvegardée par Ghislain Vieilledent et publié en accompagnement d’une publication (Vieilledent et al. 2020).\n\n\nCode\nsapm_2010 &lt;- st_read(\n  paste0(data_dir, \"Vieilledent_sapm_20101220/AP-NAP_38s.shp\"),\n  quiet = TRUE,\n  options = \"ENCODING=WINDOWS-1252\"\n) %&gt;%\n  st_transform(my_crs) %&gt;%\n  clean_pa_names_cols(name_cols = \"NOM\")\n\nwrite_rds(sapm_2010, \"data/no_id/sapm_2010.rds\")\n\ntm_shape(sapm_2010) +\n  tm_borders(col = \"purple\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Harmonisation des données SAPM</span>"
    ]
  },
  {
    "objectID": "01_sapm_data.html#evolution-du-sapm-2002-2011",
    "href": "01_sapm_data.html#evolution-du-sapm-2002-2011",
    "title": "2  Harmonisation des données SAPM",
    "section": "2.3 Evolution du SAPM 2002-2011",
    "text": "2.3 Evolution du SAPM 2002-2011\nCe jeu de données a été publié en ligne par un consultant du SAPM, avec la référence suivante : Evolution of the Madagascar Protected Area System (SAPM), Tom Allnut, https://hub.arcgis.com/content/4218737646234c7cab1c7a20e2c2489d/about\nIt is provided as a .mpk, which is in fact a zip including shapefiles. We convert to Zip and open the shapefiles.\n\n\nCode\n# Specify the directory path\ndir_path &lt;- paste0(data_dir, \"sapm_evolution_2012/commondata/data1\")\n\n# List all .shp files in the directory\nshapefile_paths &lt;- list.files(\n  path = dir_path,\n  pattern = \"\\\\.shp$\",\n  full.names = TRUE,\n  recursive = TRUE\n)\n\n# Extract the filenames without the full path\nshapefile_names &lt;- basename(shapefile_paths)\n\n# Filter shapefiles with a date in the filename (e.g., \"2008\", \"2009\", etc.)\ndate_shapefiles &lt;- shapefile_paths[str_detect(shapefile_names, \"20[0-9]{2}\")]\n\n# Load and bind the shapefiles\nsapm_evol_2001_2011 &lt;- date_shapefiles %&gt;%\n  map(\n    ~ st_read(.x, quiet = TRUE) %&gt;% # Read each shapefile\n      clean_pa_names_cols(name_cols = \"NOM\")\n  ) %&gt;% # and clean PA name\n  reduce(bind_rows) %&gt;%\n  mutate(YEAR = str_extract(YEAR, \"\\\\d{4}\")) %&gt;% # Extract the 4-digit year\n  st_transform(crs = my_crs) # On passe les données en laborde malgache\n\nsapm_evol_2001_2011 %&gt;%\n  filter(YEAR == 2002) %&gt;%\n  tm_shape() +\n  tm_borders(col = \"blue\") +\n  tm_shape(angap_2002) +\n  tm_fill(col = \"red\", alpha = 0.5)\n\n\n\n\n\n\nCode\n# Save the data to a specified path\nwrite_rds(sapm_evol_2001_2011, \"data/no_id/sapm_evol_2001_2011.rds\")\n\n\nLa superposition des limites extraites de cette série temporelle avec celles issues de la base ANGAP 2002 permet de visualiser les premiers recouvrements et écarts.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Harmonisation des données SAPM</span>"
    ]
  },
  {
    "objectID": "01_sapm_data.html#sapm-2017",
    "href": "01_sapm_data.html#sapm-2017",
    "title": "2  Harmonisation des données SAPM",
    "section": "2.4 SAPM 2017",
    "text": "2.4 SAPM 2017\nCe jeu de données nous a été remis par la FAPBM. Il s’agit visiblement des données SAPM mises à jour en mars 2017.\nL’un des points du périmètre de l’aire protégée Andrafiamena Andavakoera (index 32, sommet numéro 841) est localisé de manière aberrante, formant un papillon et entraînant des erreurs topologiques. La fonction st_make_valid retire ce point aberrant.\n\n\nCode\n# Load and transform data\nsapm_2017 &lt;- st_read(\n  paste0(data_dir, \"sapm_201703017_om_surface/sapm_201703017_om_surface.shp\"),\n  options = \"ENCODING=WINDOWS-1252\",\n  quiet = TRUE\n) %&gt;%\n  st_transform(my_crs) %&gt;%\n  st_make_valid() %&gt;%\n  clean_pa_names_cols(name_cols = c(\"SHORT_NAME\", \"FULL_NAME\"))\n\n# Save the data\nwrite_rds(sapm_2017, \"data/no_id/sapm_2017.rds\")\n\ntm_shape(sapm_2017) +\n  tm_polygons(col = \"CATEG_IUCN\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Harmonisation des données SAPM</span>"
    ]
  },
  {
    "objectID": "01_sapm_data.html#vahatra-2017",
    "href": "01_sapm_data.html#vahatra-2017",
    "title": "2  Harmonisation des données SAPM",
    "section": "2.5 Vahatra 2017",
    "text": "2.5 Vahatra 2017\n\n\nCode\nload(\"sources/AP_Vahatra.rds\")\nAP_Vahatra %&gt;%\n  st_transform(my_crs) %&gt;%\n  clean_pa_names_cols(name_cols = c(\"nom\", \"full_name\", \"nom_wdpa\")) %&gt;%\n  write_rds(\"data/no_id/vahatra.rds\")\n\n\ndans certaines entités.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Harmonisation des données SAPM</span>"
    ]
  },
  {
    "objectID": "01_sapm_data.html#sapm-2024",
    "href": "01_sapm_data.html#sapm-2024",
    "title": "2  Harmonisation des données SAPM",
    "section": "2.6 SAPM 2024",
    "text": "2.6 SAPM 2024\nCe jeu de données constitue l’état le plus récent de la base SAPM. Il inclut de nouvelles aires protégées récemment créées ainsi que des ajustements sur les périmètres existants.\n\n\nCode\n# Load shapefile\nsapm_2024 &lt;- st_read(\n  paste0(data_dir, \"Shape AP mai2023/dernier version shape_ap_28042023.shp\"),\n  quiet = TRUE\n)\n\n# Check for invalid geometries\ninvalid_geometries &lt;- sapm_2024 %&gt;%\n  st_zm(drop = TRUE, what = \"ZM\") %&gt;% # &lt;- remove Z and M before anything else\n  rownames_to_column(var = \"index\") %&gt;%\n  rowwise() %&gt;%\n  mutate(is_valid = st_is_valid(geometry)) %&gt;%\n  filter(!is_valid) %&gt;%\n  select(Index = index, Nom = SHORT_NAME, Source = path)\n\n# Create a folder to save the images\noutput_dir &lt;- \"invalid_geometry_plots\"\ndir.create(output_dir, showWarnings = FALSE)\n\n# Loop through invalid geometries and save the plots\nfor (i in seq_len(nrow(invalid_geometries))) {\n  geometry_index &lt;- invalid_geometries$Index[i]\n\n  # Generate a file name\n  plot_file &lt;- file.path(\n    output_dir,\n    paste0(\"geometry_\", geometry_index, \".png\")\n  )\n\n  # Save the plot\n  png(plot_file)\n  plot(st_geometry(sapm_2024[geometry_index, ]))\n  dev.off()\n}\n\n# Add image file paths to the invalid geometries table\ninvalid_geometries &lt;- invalid_geometries %&gt;%\n  mutate(\n    Geometrie = paste0(output_dir, \"/geometry_\", Index, \".png\")\n  )\n\n# Display the table with gt\nif (nrow(invalid_geometries) == 0) {\n  message(\"All geometries are valid.\")\n} else {\n  invalid_geometries %&gt;%\n    st_drop_geometry() %&gt;%\n    gt() %&gt;%\n    text_transform(\n      locations = cells_body(columns = Geometrie),\n      fn = function(x) {\n        local_image(filename = x, height = 100) # Embed local image with specified height\n      }\n    ) %&gt;%\n    tab_header(\n      title = \"Géométries invalides\",\n      subtitle = \"Liste des géométries invalides\"\n    )\n}\n\n\n\n\n\n\n\n\nGéométries invalides\n\n\nListe des géométries invalides\n\n\nIndex\nNom\nSource\nGeometrie\n\n\n\n\n3\nCM7B\nNA\n\n\n\n8\nMangiho\nNA\n\n\n\n12\nIVOHIBORO\nD:\\ASA AP\\FOREST SHAPE.shp\n\n\n\n22\nBehara Tranomaro\nD:\\SHAPE AP\\Shapfile vaovao\\sapm_201703017_dd\\sapm_201703017_dd.shp\n\n\n\n34\nTorotorofotsy\nD:\\SHAPE AP\\Shapfile vaovao\\sapm_201703017_dd\\sapm_201703017_dd.shp\n\n\n\n47\nAndrafiamena Andavakoera\nD:\\SHAPE AP\\Shapfile vaovao\\sapm_201703017_dd\\sapm_201703017_dd.shp\n\n\n\n50\nMenabe Antimena\nD:\\SHAPE AP\\Shapfile vaovao\\sapm_201703017_dd\\sapm_201703017_dd.shp\n\n\n\n61\nAnalalava\nD:\\SHAPE AP\\Shapfile vaovao\\sapm_201703017_dd\\sapm_201703017_dd.shp\n\n\n\n65\nMakirovana Tsihomanaomby\nD:\\SHAPE AP\\Shapfile vaovao\\sapm_201703017_dd\\sapm_201703017_dd.shp\n\n\n\n68\nPointe à Larrée\nD:\\SHAPE AP\\Shapfile vaovao\\sapm_201703017_dd\\sapm_201703017_dd.shp\n\n\n\n70\nAntrema\nD:\\SHAPE AP\\Shapfile vaovao\\sapm_201703017_dd\\sapm_201703017_dd.shp\n\n\n\n107\nPic d'Ivohibe\nD:\\SHAPE AP\\Shapfile vaovao\\sapm_201703017_dd\\sapm_201703017_dd.shp\n\n\n\n108\nRanomafana\nD:\\SHAPE AP\\Shapfile vaovao\\sapm_201703017_dd\\sapm_201703017_dd.shp\n\n\n\n112\nZahamena\nD:\\SHAPE AP\\Shapfile vaovao\\sapm_201703017_dd\\sapm_201703017_dd.shp\n\n\n\n121\nBemanevika\nD:\\SHAPE AP\\Shapfile vaovao\\sapm_201703017_dd\\sapm_201703017_dd.shp\n\n\n\n131\nCOMATSA Nord\nD:\\SHAPE AP\\Shapfile vaovao\\sapm_201703017_dd\\sapm_201703017_dd.shp\n\n\n\n133\nNord-Ifotaka\nD:\\SHAPE AP\\Shapfile vaovao\\sapm_201703017_dd\\sapm_201703017_dd.shp\n\n\n\n\n\n\n\n\nCode\n# Now try to make this valid\n\nsapm_2024_valid &lt;- sapm_2024 %&gt;%\n  st_make_valid()\n\n# Identify invalid geometries\ninvalid_geometries &lt;- sapm_2024_valid %&gt;%\n  filter(!st_is_valid(geometry))\n\n#  Print the names (or identifiers) of the invalid geometries\n# Replace 'NOM' with the actual column containing the name or identifier\nif (nrow(invalid_geometries) &gt; 0) {\n  cat(\"Invalid geometries found:\\n\")\n  print(invalid_geometries$SHORT_NAME) # Adjust this to match your column name\n} else {\n  cat(\"All geometries are valid.\\n\")\n}\n\n\nInvalid geometries found:\n[1] \"CM7B\"                     \"Mangiho\"                 \n[3] \"IVOHIBORO\"                \"Andrafiamena Andavakoera\"\n\n\nCode\n# Step 5: Filter out invalid geometries for now to plot the valid ones\nsapm_2024_valid_clean &lt;- sapm_2024_valid %&gt;%\n  filter(st_is_valid(geometry)) %&gt;%\n  clean_pa_names_cols(name_cols = c(\"Name\", \"SHORT_NAME\")) %&gt;%\n  st_transform(my_crs)\n\nwrite_rds(sapm_2024_valid_clean, \"data/no_id/sapm_2024.rds\")\n\n# Step 6: Plot the valid geometries using tmap\ntm_shape(sapm_2024_valid_clean) +\n  tm_fill(col = \"blue\", alpha = 0.5) +\n  tm_shape(sapm_2017) +\n  tm_fill(col = \"red\", alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nVieilledent, Ghislain, Marie Nourtier, Clovis Grinand, Miguel Pedrono, Alison Clausen, Tsiky Rabetrano, Jean-Roger Rakotoarijaona, et al. 2020. “It’s Not Just Poverty: Unregulated Global Market and Bad Governance Explain Unceasing Deforestation in Western Madagascar.” BioRxiv, 2020–07.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Harmonisation des données SAPM</span>"
    ]
  },
  {
    "objectID": "02_wdpa_data.html",
    "href": "02_wdpa_data.html",
    "title": "3  Exploration of WDPA historical data",
    "section": "",
    "text": "3.1 Creation WDPA global historical database 2010-2024\nNous avons développé un jeu de données spatio-temporel consolidant les données mondiales publiées par la World Database on Protected Areas (WDPA) depuis 2010. Nous avons compilé un jeu de données par an, en prenant pour référence le mois d’octobre ou, lorsque les données d’octobre n’étaient pas disponibles, le mois le plus proche.\nLes sources ont été obtenues grâce à une recherche approfondie dans les archives pour récupérer les enregistrements historiques publiés par la WDPA, puisque seules les données les plus récentes sont disponibles publiquement sur protectedplanet.net. Google Earth Engine archive les données depuis 2017, et nous avons trouvé les données pour les années 2010 à 2016 dans le dépôt public AWS de l’UNEP-WCMC.\nLes schémas des données ont évolué au fil du temps, et nous avons entrepris des efforts d’harmonisation pour aligner les attributs entre les différentes versions du jeu de données.\nLe résultat est un jeu de données comprenant 2 721 134 enregistrements annuels d’aires protégées pour le monde entier. Les géométries des aires protégées sont complexes et représentent plus de 95 % du volume des données. La déduplication de ce nombre d’observations est une opération computationnellement intensive. Nous utilisons l’algorithme Blake3, une fonction de hachage cryptographique déterministe qui produit systématiquement la même valeur de hachage pour des données identiques, ce qui le rend extrêmement efficace pour la déduplication des données. Cet algorithme, l’un des plus performants disponibles, est implémenté en C dans notre code. Les hachages produits par Blake3 servent de clés pour séparer les données attributaires des données spatiales.\nAinsi, nous avons obtenu deux fichiers parquet contenant toutes les données WDPA des 15 dernières années pour 365 592 aires protégées : un fichier contenant 2 721 134 enregistrements annuels d’attributs des aires protégées (190 Mo) et un autre contenant 1 425 023 géométries spatiales uniques (13,9 Go).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploration of WDPA historical data</span>"
    ]
  },
  {
    "objectID": "02_wdpa_data.html#creation-dune-base-wdpa-pour-madagascar-pour-2010-2024",
    "href": "02_wdpa_data.html#creation-dune-base-wdpa-pour-madagascar-pour-2010-2024",
    "title": "3  Exploration of WDPA historical data",
    "section": "3.2 Creation d’une base WDPA pour Madagascar pour 2010-2024",
    "text": "3.2 Creation d’une base WDPA pour Madagascar pour 2010-2024\nÀ partir de la base globale, nous extrayons les observations correspondant à Madagascar pour produire une base allégée dédiée à l’analyse nationale. Ce sous-ensemble est ensuite reprojeté dans le système de coordonnées local utilisé à Madagascar (Laborde, EPSG:29702).\n\n\nCode\nlibrary(tidyverse)\nlibrary(arrow)\nlibrary(geoarrow)\nlibrary(sf)\n\nmy_crs &lt;- 29702\n\n\n# Load parquet\nwdpa_mdg &lt;- read_parquet(\"sources/MDG_WDPA_Consolidated.parquet\") %&gt;%\n  mutate(geometry = st_as_sfc(geometry)) %&gt;%\n  st_as_sf() %&gt;%\n  st_transform(st_crs(my_crs))\n\nwrite_parquet(wdpa_mdg, \"data/MDG_WDPA_Consolidated.parquet\")\n\n\nNous pouvons ensuite produire des premiers indicateurs simples permettant de visualiser l’évolution du nombre et de la surface cumulée des aires protégées à Madagascar entre 2010 et 2024\n\n\nCode\n# Calculer le nombre d'aires protégées par année\nap_per_year &lt;- wdpa_mdg %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(data_year) %&gt;%\n  summarise(Number_of_APs = n())\n\n# Calculer la surface totale par année\narea_per_year &lt;- wdpa_mdg %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(data_year) %&gt;%\n  summarise(Total_Area = sum(GIS_AREA, na.rm = TRUE))\n\n# Graphique : Nombre d'aires protégées par année\nggplot(ap_per_year, aes(x = data_year, y = Number_of_APs)) +\n  geom_bar(stat = \"identity\", fill = \"blue\") +\n  labs(\n    title = \"Nombre d'aires protégées par année\",\n    x = \"Année\",\n    y = \"Nombre d'AP\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Graphique : Surface totale des aires protégées par année\nggplot(area_per_year, aes(x = data_year, y = Total_Area)) +\n  geom_bar(stat = \"identity\", fill = \"green\") +\n  labs(\n    title = \"Surface totale des aires protégées par année\",\n    x = \"Année\",\n    y = \"Surface totale (ha)\"\n  ) +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploration of WDPA historical data</span>"
    ]
  },
  {
    "objectID": "03_legal_docs.html",
    "href": "03_legal_docs.html",
    "title": "4  Création d’une base réglementaire à partir de CNLEGIS",
    "section": "",
    "text": "4.1 Objet\nLe processus de création, modification ou disparition des aires protégées passe par des lois, décrets et arrêtés officiels. Ces éléments réglementaires sont dispersés dans divers textes juridiques, que nous allons ici collecter et consolider à partir de la base du Centre National d’Information et de Documentation Legislative et Juridique (CNLEGIS). Cette instance centralise les textes réglementaires à Madagascar et propose un moteur de recherche permettant de trouver les documents pertinents en fonction de recherches full-text. Cette section détaille la méthodologie employée pour collecter, traiter et structurer ces informations.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Création d'une base réglementaire à partir de CNLEGIS</span>"
    ]
  },
  {
    "objectID": "03_legal_docs.html#extraction-des-textes-pertinents",
    "href": "03_legal_docs.html#extraction-des-textes-pertinents",
    "title": "4  Création d’une base réglementaire à partir de CNLEGIS",
    "section": "4.2 Extraction des textes pertinents",
    "text": "4.2 Extraction des textes pertinents\nLa base CNLEGIS ne propose pas d’API, donc l’extraction a dû être réalisée manuellement, à partir de requête contenant les expressions reconnues pour désigner les aires protégées. Loi N° 90-033 du 21 décembre 1990 portant Charte de l’Environnement Malgache n’établissait pas de typologies pour les aires protégées et se référait simplement aux “parcs et réserves”. Dans la Loi n° 2001-005 portant code de gestion des aires protégées établissait trois catégories : Réserve naturelle intégrale, Parc national, et Réserve spéciale. La Loi n°2008-025 portant refonte du Code de Gestion des Aires Protégées étend cette liste à 7 catégories : La Réserve Naturelle Intégrale (RNI), le Parc National (PN), la Réserve Spéciale (RS), le Parc Naturel (PNAT), le Monument Naturel (MONAT), le Paysage Harmonieux Protégé (PHP), et la Réserve de Ressources Naturelles (RRN). La Loi n°2015-005 portant refonte du Code de Gestion des Aires Protégées reprend cette même liste.\nSur cette base, on a entré les requêtes suivantes, qui ont donné le nombre de résultats indiqués ensuite :\n\naire protégée : 131\nmonument naturel : 0\nparc national : 29\nparc naturel : 1\npaysage harmonieux : 1\nréserve naturelle : 12\nréserve spéciale : 21\nréserve de ressource : 0\nprotection temporaire globale: 3\n\nLes résultats de chaque requête sont regroupées par pages contenant entre 1 et 2 réponses. Nous avons extrait les 25 pages web contenant les résultats ont été enregistrées localement. Le code ci-dessous extrait les informations pertinentes qu’elles contiennent, les nettoient et les restitue sous forme tabulaire.\n\n\nCode\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(wdpar)\n\n# Specify the source repository\ndata_dir &lt;- \"sources/\"\n# Directory containing the HTML files\ninput_dir &lt;- paste0(data_dir, \"Décrets/CNLEGIS_complet\")\n\n\n# Columns to select\nkeep_columns &lt;- c(\n  \"DATE(S)\",\n  \"DATE TEXTES\",\n  \"TEXTE(S)\",\n  \"TYPE\",\n  \"NUM TEXTE\",\n  \"NUM\",\n  \"OBJET\",\n  \"OBJET MG\",\n  \"NUM JO\",\n  \"DATE JO\",\n  \"DATE JO FR\",\n  \"PAGE JO\",\n  \"ETAT\",\n  \"NOTE(S)\",\n  \"NOTES MG\",\n  \"DOC PDF FR\",\n  \"DOC PDF MG\",\n  \"MINISTERE(S)\",\n  \"id\",\n  \"HTML FR\",\n  \"HTML MG\"\n)\nkeep_types &lt;- c(\n  \"Constitution\",\n  \"Loi constitutionnelle\",\n  \"Loi organique\",\n  \"Loi\",\n  \"Ordonnance\",\n  \"Décret\",\n  \"Arrêté\",\n  \"Circulaire\",\n  \"Décision\",\n  \"Instruction\",\n  \"Note\",\n  \"Délibération\",\n  \"Arrêt\",\n  \"Avis\",\n  \"Palmarès\",\n  \"Procès verbal\",\n  \"Exposé des motifs de la loi\",\n  \"Jugement\",\n  \"Déclaration\"\n)\n\n# Function to clean column names\nclean_column_names &lt;- function(names) {\n  names %&gt;%\n    str_replace_all(\"\\\\(|\\\\)\", \"\") %&gt;% # Remove parentheses\n    str_replace_all(\" \", \"_\") %&gt;% # Replace spaces with underscores\n    str_to_lower() # Convert to lowercase\n}\n\n\n# Function to extract the desired table from a single HTML file\nextract_table &lt;- function(file_path) {\n  # Read the HTML file\n  html &lt;- read_html(file_path)\n\n  # Extract the table\n  table &lt;- html %&gt;%\n    html_node(\"table#Lst_docs\") %&gt;%\n    html_table(fill = TRUE)\n\n  # Select the relevant columns\n  selected_columns &lt;- table %&gt;%\n    select(all_of(keep_columns)) %&gt;%\n    filter(TYPE %in% keep_types)\n\n  colnames(selected_columns) &lt;- clean_column_names(colnames(selected_columns))\n\n  # Ensure all columns are converted to text\n  selected_columns &lt;- selected_columns %&gt;%\n    mutate(across(everything(), as.character))\n\n  return(selected_columns)\n}\n\n# Get a list of all HTML files in the directory\nhtml_files &lt;- list.files(input_dir, pattern = \"\\\\.html?$\", full.names = TRUE)\n\n# Apply the extraction function to all files\nall_results &lt;- map_dfr(html_files, extract_table) %&gt;%\n  mutate(\n    date_txt = dmy(date_textes), # Convertit les dates\n    date_jo = dmy(date_jo),\n    date_jo = coalesce(date_jo, date_txt)\n  ) %&gt;% # 2 cas\n  relocate(date_txt, date_jo, .before = everything()) # place les dates au début\n\nall_results |&gt;\n  select(\n    -html_fr,\n    -html_mg,\n    -objet_mg,\n    -textes,\n    -num,\n    -doc_pdf_fr,\n    -doc_pdf_mg\n  ) |&gt;\n  DT::datatable(height = 10)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Création d'une base réglementaire à partir de CNLEGIS</span>"
    ]
  },
  {
    "objectID": "03_legal_docs.html#classification-par-type-de-réglement",
    "href": "03_legal_docs.html#classification-par-type-de-réglement",
    "title": "4  Création d’une base réglementaire à partir de CNLEGIS",
    "section": "4.3 Classification par type de réglement",
    "text": "4.3 Classification par type de réglement\nLes décisions identifiées ont des effets variés : certaines créent de nouvelles aires protégées, d’autres modifient leurs limites, tandis que d’autres prorogent leur statut. Nous appliquons des règles de typologie permettant d’assigner une catégorie à chaque texte.\n\n\nCode\n# Add typology columns\nclass_results &lt;- all_results %&gt;%\n  mutate(\n    creation_definitive = str_detect(\n      textes,\n      regex(\n        \"(?&lt;!en )(création|crétion de l'aire protégée)|instituant\",\n        ignore_case = TRUE\n      )\n    ),\n    modifiant = str_detect(\n      textes,\n      regex(\"(modifi)|(changem)|Précis\", ignore_case = TRUE)\n    ),\n    prorogation = str_detect(textes, regex(\"prorog\", ignore_case = TRUE)),\n    delegation_gestion = str_detect(\n      textes,\n      regex(\"délégation de gestion\", ignore_case = TRUE)\n    ),\n    mise_protection_temporaire = str_detect(\n      textes,\n      regex(\"protection temporaire\", ignore_case = TRUE)\n    ),\n    nomination = str_detect(textes, regex(\"nomination\", ignore_case = TRUE))\n  ) %&gt;%\n  mutate(\n    total_true = rowSums(\n      select(., creation_definitive:nomination),\n      na.rm = TRUE\n    ),\n    concatenated_true = pmap_chr(\n      select(., creation_definitive:nomination),\n      ~ paste(names(which(c(...))), collapse = \", \")\n    ),\n    .before = everything()\n  ) %&gt;%\n  filter(!(total_true == 0)) %&gt;%\n  filter(!(total_true == 1 & nomination)) %&gt;% # On enlève le copil Sydney\n  filter(!(str_detect(textes, \"fonctionnement du Comité de Pilotage\"))) %&gt;%\n  filter(!(str_detect(textes, \"fonctionnement de la Commission\"))) %&gt;%\n  filter(!(str_detect(textes, \"organisation du Comité\")))\n\n\nDeux exceptions méritent d’être relevées :\n\nUn cas important est l’Arrêté Interministériel n° 18633/2008/MEFT/MEM du 17 octobre 2008 portant mise en protection temporaire globale des sites visés par l’Arrêté interministériel n° 17914 du 18 octobre 2006 et levant la suspension de l’octroi des permis miniers et forestiers pour certains sites. Cette décision s’applique à 97 nouvelles aires protégées en cours de création et 29 sites prioritaires pour la biodiversité et la gestion forestière.\n\nCe dernier a été amendé par l’Arrêté interministériel n° 52005/2010 du 20 décembre 2010 modifiant l’arrêté interministériel Mine-Forêts n°18633 du 17 octobre 2008 portant mise en protection temporaire globale des sites visés par l’arrêté n°17914 du 18 octobre 2006 et levant la suspension de l’octroi des permis miniers et forestiers pour certains sites. Cette décision s’applique à 95 aires protégées (24 ayant déjà un statut de protection temporaire).\n\nCes deux textes concernent une liste très étendue de sites potentiels, qui ne sont pas nommément listés dans le texte. On a juste une carte (peu précise) et une somme de zones concernées (en nombre et en surface). Nous devons donc croiser ces informations avec la base SAPM pour retrouver les sites affectés.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Création d'une base réglementaire à partir de CNLEGIS</span>"
    ]
  },
  {
    "objectID": "03_legal_docs.html#appariement-des-noms-daires-protégées-entre-cnlegis-et-sapm",
    "href": "03_legal_docs.html#appariement-des-noms-daires-protégées-entre-cnlegis-et-sapm",
    "title": "4  Création d’une base réglementaire à partir de CNLEGIS",
    "section": "4.4 Appariement des noms d’aires protégées entre CNLEGIS et SAPM",
    "text": "4.4 Appariement des noms d’aires protégées entre CNLEGIS et SAPM\nLes noms extraits des textes officiels peuvent varier par rapport aux noms standardisés dans les bases spatiales (SAPM, WDPA), en raison de différences d’orthographe, de formulation, ou de précision géographique. Un nettoyage et une extraction robuste sont donc nécessaires avant tout appariement.\n\n\nCode\nclass_results &lt;- class_results %&gt;%\n  mutate(\n    pa = str_extract(\n      textes,\n      regex(\"[\\\"«](.*?)[\\\"»,]|nommée\\\\s*(.*?)[»,]\", ignore_case = TRUE)\n    ),\n    pa = ifelse(\n      is.na(pa),\n      str_extract(\n        textes,\n        regex(\"nommée\\\\s*(.*?)[\\\"]\", ignore_case = TRUE)\n      ),\n      pa\n    ),\n    pa = ifelse(\n      str_detect(textes, \"respectivement\"),\n      str_extract(\n        textes,\n        regex(\"respectivement\\\\s+(.*?)(?=\\\\.|N°|ETAT)\", ignore_case = TRUE)\n      ),\n      pa\n    ),\n    pa = str_replace(pa, \"\\\"Complexe des Aires\", \"Complexe des Aires\"),\n    pa = str_remove(pa, \"^nommée\\\\s*\"), # Supprime \"nommée\"\n    pa = str_remove(pa, \"^respectivement\\\\s*\"), # Supprime \"nommée\"\n    pa = str_remove(pa, \"(?&lt;=\\\")[^\\\"«]*$\"), # Supprime tout après le 2e guillemet\n    pa = str_remove_all(pa, \"[\\\"«»]\"), # Supprime les quotes\n    pa = str_trim(pa), # Supprime les espaces en trop\n    pa = str_remove(pa, \",$\"),\n    pa = str_remove(pa, \"^'\"),\n    .before = textes\n  )\n\n# Cleans PA name columns only (e.g. NOM, SHORT_NAME, etc.)\n# You must specify which columns to clean\nclean_pa_names_cols &lt;- function(df, name_cols) {\n  df %&gt;%\n    mutate(across(\n      all_of(name_cols),\n      ~ .x %&gt;%\n        str_replace_all(\"[\\r\\n\\t]\", \" \") %&gt;% # replace line breaks, carriage returns, tabs with space\n        str_squish() %&gt;% # collapse multiple spaces into one\n        str_trim() # remove leading/trailing whitespace\n    ))\n}\n# Clean extracted PA names (remove \\r, \\n, tabs, trim spaces)\nclass_results &lt;- clean_pa_names_cols(class_results, name_cols = \"pa\")\n\npn_rs &lt;- c(\n  \"Andohahela\",\n  \"Nosy Mangabe\",\n  \"Montagne d'Ambre\",\n  \"Ankarafantsika\",\n  \"Analamazaotra\",\n  \"Kirindy Mite\",\n  \"Tsimanampesotse\",\n  \"Mikea\",\n  \"Nosy Hara\",\n  \"Nosy Tanikely\",\n  \"Lokobe\",\n  \"Ankarafantsika\",\n  \"Tsimanampetsotsa\",\n  \"Namokora\",\n  \"Mantadia\",\n  \"Marojejy\",\n  \"Kirindy Mite\",\n  \"Befotaka Midongy\",\n  \"Zombitse-Vohibasia\",\n  \"Baie de Baly\",\n  \"Tsingy-de-Bemaraha\",\n  \"Masoala\",\n  \"Mantadia\",\n  \"Isalo\",\n  \"montagne d'Ambre\",\n  \"Lokobe\",\n  \"Ankarafantsika\",\n  \"Tsimanampetsotsa\",\n  \"Namokora\",\n  \"Marojejy\",\n  \"Tsingy-de-Bemaraha\",\n  \"Andringitra\",\n  \"Ankarafantsika\",\n  \"Ankarafantsika\",\n  \"Nosy Mangabe\",\n  \"Montagne d'Ambre\",\n  \"Manongarivo\",\n  \"Ambatovaky\",\n  \"Beza-Mahafaly\",\n  \"Cap Sainte Marie\",\n  \"Anjanaharibe-Sud\",\n  \"forêt d'Ambohitantely\",\n  \"Manombo\",\n  \"île de Mangabe\",\n  \"Ambatovavy\",\n  \"pic d'Ivohibe\",\n  \"Mangerivola\",\n  \"Manombo\",\n  \"cap Sainte-Marie\",\n  \"forêt d'Ambre\",\n  \"forêt Tampoketsa d'Analamaitso\",\n  \"Andranomena\",\n  \"Anjanaharibe-Sud\",\n  \"Ambohijanahary\",\n  \"Pointe à Larrée\"\n) |&gt;\n  unique() |&gt;\n  sort()\n\nclass_results &lt;- class_results %&gt;%\n  mutate(\n    textes = str_replace_all(textes, \"[\\r\\n]\", \" \"),\n    textes = str_replace_all(textes, \"/\", \" \"),\n    textes = str_squish(textes),\n    pa = ifelse(\n      is.na(pa),\n      map_chr(\n        textes,\n        ~ first(pn_rs[str_detect(.x, pn_rs)], default = NA_character_)\n      ),\n      pa\n    ), # for some unkown reason, \"Pointe à Larrée is not recognized\n    pa = ifelse(num_texte == \"2015-773\", \"Pointe à Larrée\", pa), # Explicit assignment\n    pa = ifelse(num_texte == \"98-376\", \"Andrigitra\", pa) # Explicit assignment\n  )\n\nclass_results &lt;- class_results %&gt;%\n  mutate(\n    # For 2008, we find the list of PA created then\n    pa = ifelse(\n      num_texte == \"18633/2008\" |\n        num_texte == \"52005/2010\" |\n        num_texte == \"9874/2013\",\n      read_rds(\"data/no_id/sapm_2010.rds\") |&gt;\n        filter(YEAR_IMPLE == \"X\") |&gt;\n        pluck(\"NOM\") |&gt;\n        paste(collapse = \", \"),\n      pa\n    )\n  )\n\n\nUne fois les noms des aires protégées concernées extraites de chaque texte, on les compare aux noms de la base SAPM. On le fait tout d’abord automatiquement avec un algorithme d’appariement approximatif (“fuzzy matching”), qui tient compte des variations d’orthographe ou des dénominations multiple, afin d’identifier les correspondances les plus probables. Nous utilisons la distance de Levenshtein proportionnelle à la longueur des chaînes afin de minimiser les erreurs d’appariement sur des noms courts.\n\n\nCode\nlibrary(stringdist)\nlibrary(fuzzyjoin)\n\n\nwdpa_mdg_2025 &lt;- wdpa_read(\"sources/WDPA_WDOECM_mar2025_Public_MDG.zip\")\n\n# Harmonization for comparison (without modifying original data)\nwdpa_clean &lt;- wdpa_mdg_2025 %&gt;%\n  mutate(cleaned_NAME = str_trim(str_to_lower(NAME)))\n\nclass_clean &lt;- class_results %&gt;%\n  mutate(cleaned_pa = str_trim(str_to_lower(pa)))\n\n# Split cells containing multiple names (both \", \" and \" et \" as separators)\nsplit_class_clean &lt;- class_clean %&gt;%\n  mutate(cleaned_pa_split = str_split(cleaned_pa, \",\\\\s*|\\\\s+et\\\\s+\")) %&gt;% # Split on commas or \" et \"\n  unnest(cleaned_pa_split) %&gt;%\n  rename(single_cleaned_pa = cleaned_pa_split) # Rename for clarity\n\n# Function to compute proportional string distance\nmatch_pa_to_wdpa &lt;- function(pa_name, wdpa_names) {\n  # Calculate proportional distances\n  distances &lt;- stringdist(\n    a = pa_name,\n    b = wdpa_names,\n    method = \"lv\" # Levenshtein distance\n  ) /\n    pmax(nchar(pa_name), nchar(wdpa_names))\n\n  # Find the best match\n  best_match_idx &lt;- which.min(distances)\n  list(\n    closest_match = wdpa_names[best_match_idx],\n    match_distance = distances[best_match_idx]\n  )\n}\n\n# Find closest matches for each split name\nconversion_table &lt;- split_class_clean %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    match_result = list(match_pa_to_wdpa(\n      single_cleaned_pa,\n      wdpa_clean$cleaned_NAME\n    )),\n    closest_match = wdpa_clean$NAME[which.min(\n      stringdist(single_cleaned_pa, wdpa_clean$cleaned_NAME) /\n        pmax(nchar(single_cleaned_pa), nchar(wdpa_clean$cleaned_NAME))\n    )],\n    match_distance = min(\n      stringdist(single_cleaned_pa, wdpa_clean$cleaned_NAME) /\n        pmax(nchar(single_cleaned_pa), nchar(wdpa_clean$cleaned_NAME))\n    )\n  ) %&gt;%\n  ungroup() %&gt;%\n  select(original_pa = single_cleaned_pa, closest_match, match_distance) %&gt;%\n  distinct(original_pa, .keep_all = TRUE)\n\n\nwritexl::write_xlsx(conversion_table, \"conversion_table.xlsx\")\n\n\nOn a à ce stade effectué une modification manuelle, correspondant à la réserve naturelle intégrale n°5 devenue parc national n°14 en 1998 est dénomée “Antsiranana” dans la base, mais il s’agit d’Andrigitra, cf. site 47 dans Goodman et al. (2018).\nCette base est envoyée vers excel et on effectue une vérification manuelle pour avoir une liste complète des rapprochements. A l’issue de ce travail, on dispose d’une liste des noms des AP tels qu’on les trouve dans les textes officiels, avec une table de correspondance indiquant le nom équivalent avec lequel elles sont enregistrées dans la base SAPM 2017.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Création d'une base réglementaire à partir de CNLEGIS</span>"
    ]
  },
  {
    "objectID": "03_legal_docs.html#intégration-des-correspondances-validées-et-finalisation-de-la-base",
    "href": "03_legal_docs.html#intégration-des-correspondances-validées-et-finalisation-de-la-base",
    "title": "4  Création d’une base réglementaire à partir de CNLEGIS",
    "section": "4.5 Intégration des correspondances validées et finalisation de la base",
    "text": "4.5 Intégration des correspondances validées et finalisation de la base\nAprès validation des correspondances, nous consolidons la base en associant chaque décision réglementaire à l’aire protégée correspondante. Cela nous permet d’établir une base réglementaire normalisée, directement exploitable pour des analyses historiques ou spatiales.\n\n\nCode\nlibrary(readxl)\n\n# Load the verified conversion table\nconversion_table_verif &lt;- read_xlsx(\"data/conversion_table_verif.xlsx\") %&gt;%\n  select(original_pa, WDPA_NAME, WDPAID) %&gt;%\n  unique() %&gt;%\n  clean_pa_names_cols(name_cols = c(\"original_pa\", \"WDPA_NAME\"))\n\n\n# Merge with split_class_clean to integrate the verified closest matches\ndecision_pa &lt;- split_class_clean %&gt;%\n  left_join(\n    conversion_table_verif,\n    by = c(\"single_cleaned_pa\" = \"original_pa\")\n  ) %&gt;%\n  distinct(pa, num_texte, .keep_all = TRUE) %&gt;%\n  select(\n    date_texte = date_txt,\n    ap_nom_texte = pa,\n    WDPA_NAME,\n    WDPAID,\n    texte = textes,\n    type_texte = type,\n    num_texte,\n    num_texte_variante = num,\n    id_texte = id,\n    objet_texte = objet,\n    type_decision = concatenated_true,\n    date_jo,\n    page_jo,\n    etat_texte = etat,\n    notes_texte = notes,\n    ministeres,\n    html_fr,\n    html_mg,\n    doc_pdf_fr,\n    doc_pdf_mg\n  )\n\nwrite_rds(decision_pa, \"data/id/legal_texts.rds\")\nzip::zip(\n  \"data/id/legal_texts.zip\",\n  files = \"data/id/legal_texts.rds\",\n  mode = \"cherry-pick\"\n)\n\n\nLa base ainsi consolidée peut servir de référence pour des travaux d’analyse juridique, historique ou spatiale sur la dynamique des aires protégées à Madagascar. Elle constitue aussi une base d’appui pour la production d’indicateurs ou la vérification des statuts réglementaires.\nNous disposons maintenant d’une base de données consolidée des décisions réglementaires, où chaque texte est relié aux aires protégées qu’il concerne. Cette base est prête à être croisée avec les données spatiales pour analyser l’évolution historique des aires protégées à Madagascar.\n\n\n\n\nGoodman, Steven M., Marie Jeanne Raherilalao, Sébastien Wohlhauser, Jean Clarck N. Rabenandrasana, Herivololona M. Rakotondratsimba, Fanja Andriamialisoa, and Malalarisoa Razafimpahanana. 2018. Les Aires Protégées Terrestres de Madagascar: Leur Histoire, Description Et Biote. Association Vahatra.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Création d'une base réglementaire à partir de CNLEGIS</span>"
    ]
  },
  {
    "objectID": "04_combination.html",
    "href": "04_combination.html",
    "title": "5  Consolidation des noms",
    "section": "",
    "text": "5.1 Traitement des métadonnées ANGAP (2002)\nNous chargeons les différents jeux de données spatiaux provenant de sources hétérogènes (ANGAP, SAPM, WDPA, Vahatra) ainsi que les données légales extraites manuellement. Ils seront harmonisés et fusionnés dans un jeu unique de référence.\nLe champ CLASSEMENT contient des références légales souvent partielles ou hétérogènes. Une extraction manuelle a permis d’en dériver les champs date_texte, type_texte, num_texte (catégories existantes dans les données CNLEGIS), et une année de statut STATUS_YR. Ces champs sont intégrés au jeu de données harmonisé.\nCode\nangap_2002 &lt;- angap_2002 |&gt; \n  rename(NAME = NOM, DESIG = TYPE_AP, IUCN_CAT = IUCN, LEGAL_TXT = CLASSEMENT)\n\nlegal_txt_content &lt;- tribble(\n  ~num_ligne, ~date_texte,        ~type_texte, ~num_texte,\n  1,          \"1958-10-28\",       \"Décret\",    \"58.10\",\n  2,          \"1997-03-02\",       \"Décret\",    \"97-141\",\n  3,          \"1958-10-28\",       \"Décret\",    \"58.08\",\n  4,          \"1982-02-12\",       \"Décret\",    \"82.078\",\n  5,          \"1970-07-21\",       \"Décret\",    \"2778-MAER/SEGREF/FOR\",\n  6,          \"1956-02-20\",       \"Décret\",    \"56.208\",\n  7,          \"1997-08-07\",       \"Décret\",    \"97.1043\",\n  8,          \"1958-10-28\",       \"Décret\",    \"58.13\",\n  9,          \"1998-10-19\",       \"Décret\",    \"98.376\",\n  10,         \"1958-10-28\",       \"Décret\",    \"58.12\",\n  11,         NA,                 NA,          NA,\n  12,         \"1956-02-20\",       \"Décret\",    \"56.208\",\n  13,         \"1997-12-18\",       \"Décret\",    \"97-1452\",\n  14,         NA,                 NA,          NA,\n  15,         \"1956-09-10\",       \"Décret\",    NA,\n  16,         \"1966-06-01\",       \"Décret\",    \"66.242\",\n  17,         \"1986-06-04\",       \"Décret\",    \"86.168\",\n  18,         \"1966-04-22\",       \"Décret\",    \"64-159\",\n  19,         \"1962-10-24\",       \"Décret\",    \"62.527\",\n  20,         \"1958-10-28\",       \"Décret\",    \"58.15\",\n  21,         \"1997-03-02\",       \"Décret\",    \"97-141\",\n  22,         \"1962-07-19\",       \"Décret\",    \"62.371\",\n  23,         \"1959-04-24\",       \"Décret\",    \"58.59\",\n  24,         \"1956-09-10\",       \"Décret\",    NA,\n  25,         \"1997-12-18\",       \"Décret\",    \"97.1453\",\n  26,         \"1966-06-01\",       \"Décret\",    \"66.242\",\n  27,         \"1989-07-25\",       \"Décret\",    \"89-216\",\n  28,         \"1989-07-25\",       \"Décret\",    \"89-216\",\n  29,         \"1958-10-28\",       \"Décret\",    \"58.10\",\n  30,         \"1956-02-20\",       \"Décret\",    \"56.208\",\n  31,         \"1962-12-05\",       \"Décret\",    \"62.637\",\n  32,         \"1956-02-20\",       \"Décret\",    \"56.208\",\n  33,         NA,                 NA,          NA,\n  34,         \"1998-05-19\",       \"Décret\",    \"98.375\",\n  35,         \"1956-02-20\",       \"Décret\",    \"56.208\",\n  36,         \"1997-03-02\",       \"Décret\",    \"97-141\",\n  37,         NA,                 NA,          NA,\n  38,         \"1997-12-18\",       \"Décret\",    \"97.1451\",\n  39,         \"1958-10-28\",       \"Décret\",    \"58.07\",\n  40,         \"1965-12-14\",       \"Décret\",    \"95.795\", \n  41,         \"1954-09-16\",       \"Décret\",    \"64.380\",\n  42,         \"1989-07-25\",       \"Décret\",    \"89-216\",\n  43,         NA,                 NA,          NA,\n  44,         \"1958-10-28\",       \"Décret\",    \"58.14\",\n  45,         \"1997-03-02\",       \"Décret\",    \"97-141\",\n  46,         \"1966-06-01\",       \"Décret\",    \"66.242\",\n  47,         \"1966-06-01\",       \"Décret\",    \"66.242\",\n  48,         \"1997-08-07\",       \"Décret\",    \"97-1045\",\n  49,         \"1966-06-01\",       \"Décret\",    \"66.242\",\n  50,         \"1966-06-01\",       \"Décret\",    \"66.242\",\n  51,         \"1997-08-07\",       \"Décret\",    \"97-1045\",\n  52,         NA,                 NA,          NA,\n  53,         \"1997-12-18\",       \"Décret\",    \"97-1454\",\n  54,         \"1997-08-07\",       \"Décret\",    \"97.1044\",\n  55,         \"1966-06-01\",       \"Décret\",    \"66.242\",\n  56,         \"1997-12-18\",       \"Décret\",    \"97-1454\"\n) %&gt;% \n  mutate(\n    STATUS_YR = case_when(\n      num_ligne %in% c(16, 46, 47, 49, 50, 55) ~ 1927,\n      TRUE ~ lubridate::year(lubridate::ymd(date_texte))\n    )\n  )\nangap_2002 &lt;- angap_2002 %&gt;%\n  bind_cols(\n    legal_txt_content %&gt;%\n      select(-num_ligne)\n  ) |&gt; \n  select(-LEGAL_TXT) |&gt; \n  mutate(dataset_id = \"ANGAP_2002\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Consolidation des noms</span>"
    ]
  },
  {
    "objectID": "04_combination.html#harmonisation-des-autres-sources-sapm-vahatra",
    "href": "04_combination.html#harmonisation-des-autres-sources-sapm-vahatra",
    "title": "5  Consolidation des noms",
    "section": "5.2 Harmonisation des autres sources (SAPM, Vahatra)",
    "text": "5.2 Harmonisation des autres sources (SAPM, Vahatra)\nLes autres jeux de données sont également harmonisés avec un sous-ensemble de variables cibles. On applique des transformations simples et un renommage standardisé pour assurer l’interopérabilité.\n\n\nCode\nsapm_2010 &lt;- sapm_2010 |&gt;\n  select(NAME = NOM, DESIG = DESCRIPTIO, MANG_AUTH = DATAADMIN) |&gt; \n  mutate(DESIG = str_replace(DESIG, \"Protge\", \"Protégée\"),\n         dataset_id = \"SAPM_2010\")\n\nsapm_evol_2001_2011 &lt;- sapm_evol_2001_2011 |&gt; \n  mutate(YEAR = as.numeric(YEAR)) |&gt; \n  select(NAME = NOM, DESIG = DESCRIPTIO, MANG_AUTH = DATAADMIN,\n         STATUS_YR = YEAR) |&gt; \n  mutate(dataset_id = \"SAPM_evol_2001-2011\")\n\nsapm_2017 &lt;- sapm_2017 |&gt;\n  mutate(DATE_CREAT = year(ymd(DATE_CREAT)),\n         MANG_AUTH = paste(GEST_1, GEST_2),\n         DAT_ST_DEF = as.character(DAT_ST_DEF)) |&gt; \n  select(NAME = SHORT_NAME, ORIG_NAME = FULL_NAME, \n         IUCN_CAT = CATEG_IUCN, STATUS_YR = DATE_CREAT,\n         date_texte = DAT_ST_DEF, MARINE = TYPE_AP, \n         GOV_TYPE = GOUVERNANC, num_text = STAT_DEF, \n         MANG_AUTH) |&gt; \n  mutate(dataset_id = \"SAPM_2017\")\n\nsapm_2024 &lt;- sapm_2024 |&gt;\n  mutate(DATE_CREAT = year(ymd(DATE_CREAT)),\n         MANG_AUTH = paste(GEST_1, GEST_2),\n         DAT_ST_DEF = as.character(DAT_ST_DEF)) |&gt; \n  select(NAME = SHORT_NAME,  \n         IUCN_CAT = CATEG_IUCN, STATUS_YR = DATE_CREAT,\n         date_texte = DAT_ST_DEF, MARINE = TYPE_AP, \n         GOV_TYPE = GOUVERNANC, num_text = STAT_DEF, \n         MANG_AUTH) |&gt; \n  mutate(dataset_id = \"SAPM_2024\")\n\nvahatra &lt;- vahatra |&gt; \n  mutate(STATUS_YR = year(ymd(date_creation)),\n         MANG_AUTH = paste(gest_1, gest_2),\n         date_texte = ifelse(is.na(date_modification), date_creation,\n                             paste(date_creation, date_modification, sep = \";\"))) |&gt; \n  select(NAME = nom, IUCN_CAT = cat_iucn, ORIG_NAME = full_name,\n         num_texte = creation, MANG_AUTH, STATUS_YR, date_texte) |&gt; \n  mutate(dataset_id = \"Vahatra\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Consolidation des noms</span>"
    ]
  },
  {
    "objectID": "04_combination.html#fusion-dans-un-jeu-consolidé",
    "href": "04_combination.html#fusion-dans-un-jeu-consolidé",
    "title": "5  Consolidation des noms",
    "section": "5.3 Fusion dans un jeu consolidé",
    "text": "5.3 Fusion dans un jeu consolidé\nTous les jeux de données sont fusionnés dans un unique tableau avec un schéma unifié.\n\n\nCode\nall_PAs &lt;- bind_rows(\n  angap_2002,\n  sapm_2010,\n  sapm_2017,\n  sapm_2024,\n  sapm_evol_2001_2011,\n  vahatra\n)\n\n\nAppariement spatial avec la WDPA\nOn utilise une fonction pa_match() pour identifier les aires protégées correspondantes dans la WDPA, sur la base de la proximité spatiale et du taux de recouvrement géométrique.\n\n\nCode\n# Match protected areas from x to y based on centroid distance and spatial overlap.\n# Returns the original x with best matches from y, using overlap thresholds and ranking.\n# Adds matched name, ID, and overlap percentage to x.\npa_match &lt;- function(x, y,\n                     pa_name_x = \"NAME\",\n                     pa_name_y = \"NAME\",\n                     pa_id_y = \"WDPAID\",\n                     threshold_strong_match = 0.35,\n                     threshold_weak_match = 0.1) {\n  \n  # Harmonize CRS and fix geometry\n  y &lt;- st_transform(y, st_crs(x)) %&gt;%\n    st_make_valid()\n  \n  # Prepare x and y with centroids\n  x_aug &lt;- x %&gt;%\n    mutate(index_x = row_number(),\n           name_x = .data[[pa_name_x]],\n           centroid_x = st_centroid(geometry))\n  \n  y_aug &lt;- y %&gt;%\n    mutate(index_y = row_number(),\n           name_y = .data[[pa_name_y]],\n           id_y = .data[[pa_id_y]],\n           centroid_y = st_centroid(geometry))\n  \n  # Find 3 closest y for each x\n  x_matches &lt;- x_aug %&gt;%\n    rowwise() %&gt;%\n    mutate(\n      closest_y = list(\n        y_aug %&gt;%\n          st_drop_geometry() %&gt;%\n          mutate(dist = as.numeric(st_distance(centroid_x, centroid_y))) %&gt;%\n          arrange(dist) %&gt;%\n          slice_head(n = 2) %&gt;%\n          select(index_y, dist)\n      )\n    ) %&gt;%\n    unnest(closest_y)\n  \n  # Compute spatial overlaps\n  matches_with_overlap &lt;- x_matches %&gt;%\n    rowwise() %&gt;%\n    mutate(\n      overlap = tryCatch({\n        inter &lt;- st_intersection(geometry, y_aug$geometry[index_y])\n        if (length(inter) &gt; 0) {\n          round(as.numeric(st_area(inter) / st_area(geometry)), 3)\n        } else {\n          0\n        }\n      }, error = function(e) 0)\n    ) %&gt;%\n    ungroup() %&gt;%\n    mutate(\n      name_y = y_aug[[pa_name_y]][index_y],\n      id_y = y_aug[[pa_id_y]][index_y]\n    ) %&gt;%\n    st_drop_geometry()\n  \n  # Rank and apply selection logic\n  best_matches &lt;- matches_with_overlap %&gt;%\n    group_by(index_x) %&gt;%\n    arrange(desc(overlap), dist) %&gt;%\n    mutate(rank = row_number()) %&gt;%\n    slice(1:2) %&gt;%\n    summarise(\n      index_x = first(index_x),\n      name_y = case_when(\n        n() &gt;= 2 && overlap[2] &gt; threshold_strong_match && overlap[2] &gt; overlap[1] ~ name_y[2],\n        overlap[1] &lt; threshold_weak_match ~ NA_character_,\n        TRUE ~ name_y[1]\n      ),\n      id_y = case_when(\n        n() &gt;= 2 && overlap[2] &gt; threshold_strong_match && overlap[2] &gt; overlap[1] ~ id_y[2],\n        overlap[1] &lt; threshold_weak_match ~ NA_integer_,\n        TRUE ~ id_y[1]\n      ),\n      overlap_y = case_when(\n        n() &gt;= 2 && overlap[2] &gt; threshold_strong_match && overlap[2] &gt; overlap[1] ~ overlap[2],\n        overlap[1] &lt; threshold_weak_match ~ NA_real_,\n        TRUE ~ overlap[1]\n      ),\n      .groups = \"drop\"\n    )\n  \n  # Join results back to x\n  output &lt;- x_aug %&gt;%\n    left_join(best_matches, by = \"index_x\") %&gt;%\n    select(-index_x, -name_x, -centroid_x)\n  \n  return(output)\n}\n\n\nall_PAs_matched &lt;- pa_match(all_PAs, wdpa_mdg_2025)\n\nall_PAs_unmatched &lt;- all_PAs_matched %&gt;%\n  filter(is.na(name_y))\n\nwords_to_exlcude &lt;- c(\"tsingy de\",\n                      \"corridor entre parcelles i et ii d'\",\n                      \"corridor\", \n                      \"extension\",\n                      \"for[eê]t d['’]?\", \"aire protégée( d['’]?)?\",\n                      \"androka\",\n                      \"Maromena\"\n)\n\n# Enhanced fallback matching function for protected areas\npa_match_fallback_name &lt;- function(x, y,\n                                   pa_name_x = \"NAME\",\n                                   pa_name_y = \"NAME\",\n                                   pa_id_y = \"WDPAID\",\n                                   remove_terms,\n                                   min_stringdist = 0.08) {\n  \n  # Clean and normalize names\n  clean_name &lt;- function(s) {\n    pattern &lt;- str_c(remove_terms, collapse = \"|\")\n    s %&gt;%\n      str_to_lower() %&gt;%\n      str_replace_all(pattern, \"\") %&gt;%\n      str_squish()\n  }\n  \n  # Create index for reinsertion\n  x &lt;- x %&gt;% mutate(row_index = row_number())\n  \n  # Extract unmatched rows\n  x_unmatched &lt;- x %&gt;%\n    filter(is.na(name_y)) %&gt;%\n    mutate(name_x_raw = .data[[pa_name_x]],\n           name_x_clean = clean_name(name_x_raw)) %&gt;%\n    distinct(name_x_clean, .keep_all = TRUE) %&gt;%\n    mutate(index_x = row_number())\n  \n  # Prepare y\n  y_tbl &lt;- y %&gt;%\n    st_drop_geometry() %&gt;%\n    mutate(name_y_raw = .data[[pa_name_y]],\n           name_y_clean = clean_name(name_y_raw),\n           id_y = .data[[pa_id_y]],\n           index_y = row_number())\n  \n  # String distance matrix\n  distance_matrix &lt;- stringdistmatrix(x_unmatched$name_x_clean,\n                                      y_tbl$name_y_clean,\n                                      method = \"jw\", p = 0.1)\n  \n  # Get best match per unmatched row\n  closest_matches &lt;- tibble(\n    index_x = rep(x_unmatched$index_x, times = nrow(y_tbl)),\n    index_y = rep(y_tbl$index_y, each = nrow(x_unmatched)),\n    dist = as.vector(distance_matrix)\n  ) %&gt;%\n    group_by(index_x) %&gt;%\n    slice_min(dist, n = 1, with_ties = FALSE) %&gt;%\n    ungroup()\n  \n  # Join metadata\n  matched &lt;- closest_matches %&gt;%\n    left_join(x_unmatched, by = \"index_x\") %&gt;%\n    left_join(y_tbl, by = \"index_y\") %&gt;%\n    filter(dist &lt; min_stringdist)\n  \n  # Compute overlap with matched geometry\n  matched &lt;- matched %&gt;%\n    rowwise() %&gt;%\n    mutate(overlap_y = tryCatch({\n      inter &lt;- st_intersection(geometry, y$geometry[index_y])\n      if (length(inter) &gt; 0) round(as.numeric(st_area(inter) / st_area(geometry)), 3) else 0\n    }, error = function(e) NA_real_)) %&gt;%\n    ungroup()\n  \n  # Replace original rows\n  x_updated &lt;- x %&gt;%\n    left_join(\n      matched %&gt;%\n        select(row_index, name_y = name_y_raw, id_y   = id_y.y, overlap_y, \n               stringdist = dist),\n      by = \"row_index\"\n    ) %&gt;%\n    mutate(\n      name_y = coalesce(name_y.y, name_y.x),\n      id_y = coalesce(id_y.y, id_y.x),\n      overlap_y = coalesce(overlap_y.y, overlap_y.x),\n      stringdist = if (\"stringdist.x\" %in% names(.)) coalesce(stringdist, stringdist.x) else stringdist\n    ) %&gt;%\n    select(-row_index, -ends_with(\".x\"), -ends_with(\".y\"), -stringdist)\n  \n  return(x_updated)\n} \n\n# Perform spatial matching\nall_PAs_matched &lt;- pa_match_fallback_name(x  = all_PAs_matched, \n                                          y = wdpa_mdg_2025,\n                                          remove_terms = words_to_exlcude) %&gt;%\n  rename(WDPA_NAME = name_y, WDPAID = id_y, overlap_WPDA = overlap_y)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Consolidation des noms</span>"
    ]
  },
  {
    "objectID": "04_combination.html#ajout-des-données-légales-issues-des-textes-officiels",
    "href": "04_combination.html#ajout-des-données-légales-issues-des-textes-officiels",
    "title": "5  Consolidation des noms",
    "section": "5.4 Ajout des données légales issues des textes officiels",
    "text": "5.4 Ajout des données légales issues des textes officiels\nOn ajoute les aires protégées mentionnées dans les décrets à travers le jeu legal contenant les données CNLEGIS. Ces données ne comportent pas de géométrie, aussi on en ajoute une vide afin de pouvoir les consolider dans la même table.\n\n\nCode\n# Format to be compatible with the spatial data\nlegal_conso &lt;- legal %&gt;%\n  mutate(\n    NAME = ap_nom_texte,\n    dataset_id = \"CNLEGIS_2024\",\n    date_texte = as.character(date_texte),\n    geometry = st_sfc(rep(st_geometrycollection(), n()), crs = 4326)) |&gt; \n  select(NAME, dataset_id, date_texte, type_texte, num_texte, type_decision, WDPA_NAME, WDPAID, geometry) %&gt;%\n  st_as_sf()\n\nall_PAs_matched &lt;- all_PAs_matched %&gt;%\n  st_transform(st_crs(legal_conso)) %&gt;%\n  bind_rows(legal_conso)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Consolidation des noms</span>"
    ]
  },
  {
    "objectID": "04_combination.html#export-final",
    "href": "04_combination.html#export-final",
    "title": "5  Consolidation des noms",
    "section": "5.5 Export final",
    "text": "5.5 Export final\nOn ajoute enfin les données WDPA de mars 2025. Le jeu consolidé est exporté dans des formats interopérables (.parquet et .rds).\n\n\nCode\nall_PAs_conso &lt;- wdpa_mdg_2025 |&gt; \n  mutate(dataset_id = \"WDPA_2025\") |&gt; \n  st_transform(st_crs(all_PAs_matched))  |&gt; \n  bind_rows(all_PAs_matched)\nall_PAs_conso |&gt; \n  write_parquet(\"data/id/all_PAs_conso.parquet\")\nall_PAs_conso |&gt; \n  write_rds(\"data/id/all_PAs_conso.rds\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Consolidation des noms</span>"
    ]
  },
  {
    "objectID": "06_use_cases.html",
    "href": "06_use_cases.html",
    "title": "7  Cas d’usage",
    "section": "",
    "text": "7.1 Fiabiliser WDPA\nOutre l’exploration AP par AP, ces données permettent de servir plusieurs cas d’usages.\n[Inclure un exemple sous chaque titre]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cas d'usage</span>"
    ]
  },
  {
    "objectID": "06_use_cases.html#fiabiliser-wdpa",
    "href": "06_use_cases.html#fiabiliser-wdpa",
    "title": "7  Cas d’usage",
    "section": "",
    "text": "7.1.1 Identifier et corriger des erreurs dans la base WDPA actuelle\n\nDates erronées ou absentes\nMauvais statut ou désignation\nPérimètres inexacts ou doublons\n\n\n\n7.1.2 Vérifier la robustesse d’études s’appuyant sur des bases WDPA passées\n\n\n7.1.3 Mesurer l’ampleur des améliorations apportées\n\nDisposer d’un indicateur valorisant l’effort de fiabilisation des données",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cas d'usage</span>"
    ]
  },
  {
    "objectID": "06_use_cases.html#enrichir-wdpa",
    "href": "06_use_cases.html#enrichir-wdpa",
    "title": "7  Cas d’usage",
    "section": "7.2 Enrichir WDPA",
    "text": "7.2 Enrichir WDPA\n\n7.2.1 Histoire légale plus complexe : décrets temporaires et changements de statut\n\nIntégration des protections temporaires, absentes de la WDPA\nTraçabilité des reclassements ou abandons\n\n\n\n7.2.2 Changements de statuts\n\nPassage de réserve spéciale à parc national, ou inversement\nCas de statuts abrogés ou modifiés\n\n\n\n7.2.3 Périmètres internes aux aires protégées\n\nAjout de périmètres partiels ou d’extensions intermédiaires\nDocumentation des aires non encore spatialement représentées",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cas d'usage</span>"
    ]
  },
  {
    "objectID": "06_use_cases.html#documenter-lhistorique-réglementaire-réel",
    "href": "06_use_cases.html#documenter-lhistorique-réglementaire-réel",
    "title": "7  Cas d’usage",
    "section": "7.3 Documenter l’historique réglementaire réel",
    "text": "7.3 Documenter l’historique réglementaire réel\n\n7.3.1 Restituer les étapes d’évolution d’une aire protégée\n\n\n7.3.2 Associer chaque polygone à une source réglementaire vérifiable\n\n\n7.3.3 Repérer les aires protégées documentées dans les textes mais absentes de WDPA",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cas d'usage</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html",
    "href": "07_subdivisions.html",
    "title": "8  Validation des données de subdivision",
    "section": "",
    "text": "8.1 Alaotra\nOn charge d’abord la base de données WDPA, avec laquelle on viendra comparer les résultats.\nOn a 167 AP terrestres et marines dans la base WDPA, version de novembre 2025.\nPour cette AP comme pour les précédente, on va comparer les données WDPA avec celles transmises au MEDD.\nCode\nwdpa_alaotra &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, \"Alaotra\")) |&gt; # détecte l'AP et le site RAMSAR\n  filter(str_detect(DESIG, \"Ramsar\", negate = TRUE)) # on enlève le site Ramsar\n\nsub_alaotra &lt;- st_read(\n  \"data/subdivisions/Alaotra/shapefile/Limite_AP_Alaotra.shp\",\n  quiet = TRUE\n)\n\ntm_shape(wdpa_alaotra) +\n  tm_borders(col = \"red\") +\n  tm_shape(sub_alaotra) +\n  tm_borders(col = \"blue\")\nLes deux données semblent très proches l’une de l’autre, il est peu vraisemblable que les données fournies correspondent à une subdivision interne. A moins que l’AP soit considérée comme une subdivision interne du site Ramsar?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#allée-des-baobabs",
    "href": "07_subdivisions.html#allée-des-baobabs",
    "title": "8  Validation des données de subdivision",
    "section": "8.2 Allée des Baobabs",
    "text": "8.2 Allée des Baobabs\nLe jeu de données Allée des Baobabs contient de nombeux shapefiles.\n\n\nCode\nlist.files(\n  \"data/subdivisions/Allée de Baobabs/Zonage ADB\",\n  pattern = \"\\\\.shp\"\n) |&gt;\n  cat(sep = \"\\n\")\n\n\nLimite AP ADB.shp\nMares temporaire.shp\nNoyau dur.shp\nZone Agroforesterie.shp\nZone d'habitation.shp\nZone d'occupation contrôler.shp\nZone de paturage.shp\nZone de reboisement.shp\nZone de service touristique.shp\n\n\nOn les charge et on les superpose aux données WPDA.\n\n\nCode\nwdpa_baobabs &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, \"Baobabs\"))\n\nsub_baobabs_limites &lt;- st_read(\n  \"data/subdivisions/Allée de Baobabs/Zonage ADB/Limite AP ADB.shp\",\n  quiet = TRUE\n) |&gt;\n  st_transform(st_crs(wdpa_baobabs))\nsub_baobabs_noyau_dur &lt;- st_read(\n  \"data/subdivisions/Allée de Baobabs/Zonage ADB/Noyau dur.shp\",\n  quiet = TRUE\n) |&gt;\n  st_transform(st_crs(wdpa_baobabs))\nsub_baobabs_agroforesterie &lt;- st_read(\n  \"data/subdivisions/Allée de Baobabs/Zonage ADB/Zone Agroforesterie.shp\",\n  quiet = TRUE\n) |&gt;\n  st_transform(st_crs(wdpa_baobabs))\nsub_baobabs_habitation &lt;- st_read(\n  \"data/subdivisions/Allée de Baobabs/Zonage ADB/Zone d'habitation.shp\",\n  quiet = TRUE\n) |&gt;\n  st_transform(st_crs(wdpa_baobabs))\nsub_baobabs_occupation_controlee &lt;- st_read(\n  \"data/subdivisions/Allée de Baobabs/Zonage ADB/Zone d'occupation contrôler.shp\",\n  quiet = TRUE\n) |&gt;\n  st_transform(st_crs(wdpa_baobabs))\nsub_baobabs_paturages &lt;- st_read(\n  \"data/subdivisions/Allée de Baobabs/Zonage ADB/Zone de paturage.shp\",\n  quiet = TRUE\n) |&gt;\n  st_transform(st_crs(wdpa_baobabs))\nsub_baobabs_reboisement &lt;- st_read(\n  \"data/subdivisions/Allée de Baobabs/Zonage ADB/Zone de reboisement.shp\",\n  quiet = TRUE\n) |&gt;\n  st_transform(st_crs(wdpa_baobabs))\nsub_baobabs_tourisme &lt;- st_read(\n  \"data/subdivisions/Allée de Baobabs/Zonage ADB/Zone de service touristique.shp\",\n  quiet = TRUE\n) |&gt;\n  st_transform(st_crs(wdpa_baobabs))\n\ntm_shape(wdpa_baobabs) +\n  tm_borders(col = \"red\") +\n  tm_shape(sub_baobabs_limites) +\n  tm_borders(col = \"blue\") +\n  tm_shape(sub_baobabs_noyau_dur) +\n  tm_polygons(col = \"green\", fill = \"green\") +\n  tm_shape(sub_baobabs_agroforesterie) +\n  tm_borders(col = \"brown\") +\n  tm_shape(sub_baobabs_habitation) +\n  tm_borders(col = \"orange\") +\n  tm_shape(sub_baobabs_occupation_controlee) +\n  tm_borders(col = \"purple\") +\n  tm_shape(sub_baobabs_paturages) +\n  tm_borders(col = \"grey\") +\n  tm_shape(sub_baobabs_reboisement) +\n  tm_borders(col = \"lightgreen\") +\n  tm_shape(sub_baobabs_tourisme) +\n  tm_borders(col = \"pink\") +\n  tm_add_legend(\n    type = \"lines\",\n    labels = c(\n      \"WDPA\",\n      \"Limite\",\n      \"Noyau dur\",\n      \"Agroforesterie\",\n      \"Habitation\",\n      \"Occupation controlée\",\n      \"Paturages\",\n      \"reboisement\",\n      \"Tourisme\"\n    ),\n    col = c(\n      \"red\",\n      \"blue\",\n      \"green\",\n      \"brown\",\n      \"orange\",\n      \"purple\",\n      \"grey\",\n      \"lightgreen\",\n      \"pink\"\n    )\n  ) +\n  tm_add_legend(\n    type = \"fill\",\n    labels = c(\n      \"Noyau dur\"\n    ),\n    col = c(\n      \"lightgreen\"\n    )\n  )\n\n\n\n\n\n\n\n\nComme les périmètres se recoupent, on ajoute un remplissage pour le noyau dur. On voit que le noyau dur occupe une portion plus restreinte de l’AP comparé à d’autres. On observe un décallage de la limite extérieure de l’AP entre les données transmises au MEDD et les données WDPA. Cela ne semble pas être un problème de projection.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#ambatofostsy",
    "href": "07_subdivisions.html#ambatofostsy",
    "title": "8  Validation des données de subdivision",
    "section": "8.3 Ambatofostsy",
    "text": "8.3 Ambatofostsy\nIl y a cinq shapefiles différents pour Ambatofotsy:\n\n\nCode\nlist.files(\n  \"data/subdivisions/Ambatofotsy\",\n  pattern = \"\\\\.shp\"\n) |&gt;\n  cat(sep = \"\\n\")\n\n\ndélim_afotsy_wgs84.shp\ndina30m_afotsy_wgs84.shp\ndu_afotsy_wgs84.shp\nnd_afotsy_wgs84.shp\nzuc_afotsy_wgs84.shp\n\n\n\n\nCode\nwdpa_ambatofotsy &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, regex(\"Ambatofotsy\", ignore_case = TRUE)))\n\nsub_ambatofotsy_limite &lt;- st_read(\n  \"data/subdivisions/Ambatofotsy/délim_afotsy_wgs84.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_ambatofotsy))\n\nsub_ambatofotsy_dina30m &lt;- st_read(\n  \"data/subdivisions/Ambatofotsy/dina30m_afotsy_wgs84.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_ambatofotsy))\n\nsub_ambatofotsy_du &lt;- st_read(\n  \"data/subdivisions/Ambatofotsy/du_afotsy_wgs84.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_ambatofotsy))\n\nsub_ambatofotsy_nd &lt;- st_read(\n  \"data/subdivisions/Ambatofotsy/nd_afotsy_wgs84.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_ambatofotsy))\n\nsub_ambatofotsy_zuc &lt;- st_read(\n  \"data/subdivisions/Ambatofotsy/zuc_afotsy_wgs84.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_ambatofotsy))\n\ntm_shape(wdpa_ambatofotsy) +\n  tm_borders(col = \"red\", lwd = 2) +\n  tm_shape(sub_ambatofotsy_limite) +\n  tm_borders(col = \"blue\") +\n  tm_shape(sub_ambatofotsy_nd) +\n  tm_polygons(col = \"green\", fill = \"green\") +\n  tm_shape(sub_ambatofotsy_du) +\n  tm_borders(col = \"brown\") +\n  tm_shape(sub_ambatofotsy_nd) +\n  tm_borders(col = \"purple\") +\n  tm_shape(sub_ambatofotsy_zuc) +\n  tm_borders(col = \"orange\") +\n  tm_add_legend(\n    type = \"lines\",\n    labels = c(\"WDPA\", \"Limite\", \"ND\", \"DU\", \"Dina 30m\", \"ZUC\"),\n    col = c(\"red\", \"blue\", \"green\", \"brown\", \"purple\", \"orange\")\n  ) +\n  tm_add_legend(\n    type = \"fill\",\n    labels = c(\"ND\"),\n    col = c(\"lightgreen\")\n  )\n\n\n\n\n\n\n\n\nTout semble cohérent, hormis un léger décallage des périmètres entre les données transmises au MEDD et les données WDPA.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#ambohidray",
    "href": "07_subdivisions.html#ambohidray",
    "title": "8  Validation des données de subdivision",
    "section": "8.4 Ambohidray",
    "text": "8.4 Ambohidray\n\n\nCode\nwdpa_ambohidray &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, \"Ambohidray\"))\n\nsub_ambohidray_limite &lt;- st_read(\n  \"data/subdivisions/Ambohidray/shapefiles/Veritable_Limite_Amb.shp\",\n  quiet = TRUE\n)\n\nsub_ambohidray_tampon &lt;- st_read(\n  \"data/subdivisions/Ambohidray/shapefiles/Zone tampon.shp\",\n  quiet = TRUE\n)\n\nsub_ambohidray_recherche &lt;- st_read(\n  \"data/subdivisions/Ambohidray/shapefiles/Zone de recherche.shp\",\n  quiet = TRUE\n)\n\nsub_ambohidray_tourisme &lt;- st_read(\n  \"data/subdivisions/Ambohidray/shapefiles/Zone écoutouristique.shp\",\n  quiet = TRUE\n)\n\ntm_shape(wdpa_baobabs) +\n  tm_borders(col = \"red\") +\n  tm_shape(sub_baobabs_limites) +\n  tm_borders(col = \"blue\") +\n  tm_shape(sub_baobabs_noyau_dur) +\n  tm_borders(col = \"green\") +\n  tm_shape(sub_baobabs_agroforesterie) +\n  tm_borders(col = \"brown\") +\n  tm_shape(sub_baobabs_habitation) +\n  tm_borders(col = \"orange\") +\n  tm_shape(sub_baobabs_occupation_controlee) +\n  tm_borders(col = \"purple\") +\n  tm_shape(sub_baobabs_paturages) +\n  tm_borders(col = \"grey\") +\n  tm_shape(sub_baobabs_reboisement) +\n  tm_borders(col = \"lightgreen\") +\n  tm_shape(sub_baobabs_tourisme) +\n  tm_borders(col = \"pink\") +\n  tm_add_legend(\n    type = \"lines\",\n    labels = c(\n      \"WDPA\",\n      \"Limite\",\n      \"Noyau dur\",\n      \"Agroforesterie\",\n      \"Habitation\",\n      \"Occupation controlée\",\n      \"Paturages\",\n      \"Reboisement\",\n      \"Tourisme\"\n    ),\n    col = c(\n      \"red\",\n      \"blue\",\n      \"green\",\n      \"brown\",\n      \"orange\",\n      \"purple\",\n      \"grey\",\n      \"lightgreen\",\n      \"pink\"\n    )\n  )\n\n\n\n\n\n\n\nLes données semblent cohérentes. On observe un décalage par rapport aux limites contenues dans WDPA.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#ambondrobe",
    "href": "07_subdivisions.html#ambondrobe",
    "title": "8  Validation des données de subdivision",
    "section": "8.5 Ambondrobe",
    "text": "8.5 Ambondrobe\n\n\nCode\nwdpa_ambondrobe &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, \"Ambondrobe\")) |&gt; # détecte l'AP et le site RAMSAR\n  filter(str_detect(DESIG, \"Ramsar\", negate = TRUE)) # on enlève le site Ramsar\n\nsub_ambondrobe &lt;- st_read(\n  \"data/subdivisions/Ambondrobe/shapefile/Lim_AP_Ambondrobe.shp\",\n  quiet = TRUE\n)\n\ntm_shape(wdpa_ambondrobe) +\n  tm_borders(col = \"red\") +\n  tm_shape(sub_ambondrobe) +\n  tm_borders(col = \"blue\")\n\n\n\n\n\n\nLes deux données semblent très proches l’une de l’autre, il est peu vraisemblable que les données fournies correspondent à une subdivision interne. A moins que l’AP soit considérée comme une subdivision interne du site Ramsar?\n## Ampotaka Ankorabe\n\n\nCode\nlist.files(\n  \"data/subdivisions/Ampotaka Ankorabe\",\n  pattern = \"\\\\.shp\"\n) |&gt;\n  cat(sep = \"\\n\")\n\n\ndel_zonage_ampotakankorabe.shp\ndel_zonage_ampotakankorabe.shp.ASUS-X541UV.4484.13100.sr.lock\nDina30m_AmpotakaAnkorabe.shp\nDU_AmpotakaAnkorabe.shp\nND_AmpotakaAnkorabe.shp\nZUC_AmpotakaAnkorabe.shp\n\n\n\n\nCode\nwdpa_ampotaka &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, regex(\"Ampotaka\", ignore_case = TRUE))) |&gt;\n  filter(str_detect(NAME, regex(\"Ankorabe\", ignore_case = TRUE)))\n\nsub_ampotaka_limite &lt;- st_read(\n  \"data/subdivisions/Ampotaka Ankorabe/del_zonage_ampotakankorabe.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_ampotaka))\n\nsub_ampotaka_dina30m &lt;- st_read(\n  \"data/subdivisions/Ampotaka Ankorabe/Dina30m_AmpotakaAnkorabe.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_ampotaka))\n\nsub_ampotaka_du &lt;- st_read(\n  \"data/subdivisions/Ampotaka Ankorabe/DU_AmpotakaAnkorabe.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_ampotaka))\n\n\nRe-reading with feature count reset from 3 to 1\n\n\nCode\nsub_ampotaka_nd &lt;- st_read(\n  \"data/subdivisions/Ampotaka Ankorabe/ND_AmpotakaAnkorabe.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_ampotaka))\n\nsub_ampotaka_zuc &lt;- st_read(\n  \"data/subdivisions/Ampotaka Ankorabe/ZUC_AmpotakaAnkorabe.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_ampotaka))\n\n\nRe-reading with feature count reset from 3 to 2\n\n\nCode\ntm_shape(wdpa_ampotaka) +\n  tm_borders(col = \"red\", lwd = 2) +\n  tm_shape(sub_ampotaka_limite) +\n  tm_borders(col = \"blue\") +\n  tm_shape(sub_ampotaka_nd) +\n  tm_polygons(col = \"green\", fill = \"green\") +\n  tm_shape(sub_ampotaka_du) +\n  tm_borders(col = \"brown\") +\n  tm_shape(sub_ampotaka_dina30m) +\n  tm_borders(col = \"purple\") +\n  tm_shape(sub_ampotaka_zuc) +\n  tm_borders(col = \"orange\") +\n  tm_add_legend(\n    type = \"lines\",\n    labels = c(\"WDPA\", \"Limite\", \"ND\", \"DU\", \"Dina 30m\", \"ZUC\"),\n    col = c(\"red\", \"blue\", \"green\", \"brown\", \"purple\", \"orange\")\n  ) +\n  tm_add_legend(\n    type = \"fill\",\n    labels = c(\"ND\"),\n    col = c(\"lightgreen\")\n  )\n\n\n\n\n\n\n\n\nOn observe un décallage entre les frontières externes de l’AP selon les sources (WDPA vs. envoi MEDD). On a bien un noyau dur, mais il dépasse du périmètre extérieur WDPA.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#analabe-betanantanana",
    "href": "07_subdivisions.html#analabe-betanantanana",
    "title": "8  Validation des données de subdivision",
    "section": "8.6 Analabe Betanantanana",
    "text": "8.6 Analabe Betanantanana\n\n\nCode\nlist.files(\n  \"data/subdivisions/Analabe Betanantanana\",\n  pattern = \"\\\\.shp\"\n) |&gt;\n  cat(sep = \"\\n\")\n\n\nanalabe lim project.shp\nNoyau dur Analabe.shp\n\n\n\n\nCode\nwdpa_analabe &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, regex(\"Analabe\", ignore_case = TRUE)))\n\nsub_analabe_limite &lt;- st_read(\n  \"data/subdivisions/Analabe Betanantanana/analabe lim project.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_analabe))\n\nsub_analabe_nd &lt;- st_read(\n  \"data/subdivisions/Analabe Betanantanana/Noyau dur Analabe.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_analabe))\n\ntm_shape(wdpa_analabe) +\n  tm_borders(col = \"red\", lwd = 2) +\n  tm_shape(sub_analabe_limite) +\n  tm_borders(col = \"blue\") +\n  tm_shape(sub_analabe_nd) +\n  tm_polygons(col = \"green\", fill = \"green\") +\n  tm_add_legend(\n    type = \"lines\",\n    labels = c(\"WDPA\", \"Limite\", \"ND\"),\n    col = c(\"red\", \"blue\", \"green\")\n  ) +\n  tm_add_legend(\n    type = \"fill\",\n    labels = c(\"ND\"),\n    col = c(\"lightgreen\")\n  )\n\n\n\n\n\n\n\n\nOn observe un décalage important entre la limite de l’AP transmise par le gestionnaire et celle disponible dans WDPA (plus petite) : elles ne se recouvent que partiellement. Le noyau dur fourni ne recoupe que partiellement la limite WDPA.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#analalava",
    "href": "07_subdivisions.html#analalava",
    "title": "8  Validation des données de subdivision",
    "section": "8.7 Analalava",
    "text": "8.7 Analalava\n\n\nCode\nlist.files(\n  \"data/subdivisions/Analalava\",\n  pattern = \"\\\\.shp\"\n) |&gt;\n  cat(sep = \"\\n\")\n\n\nanalalava limite project.shp\nND analalava.shp\n\n\n\n\nCode\nwdpa_analalava &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, regex(\"Analalava\", ignore_case = TRUE))) |&gt;\n  filter(IUCN_CAT == \"VI\")\n\nsub_analalava_limite &lt;- st_read(\n  \"data/subdivisions/Analalava/analalava limite project.shp\",\n  quiet = TRUE\n) |&gt;\n  st_transform(st_crs(wdpa_analalava))\n\nsub_analalava_nd &lt;- st_read(\n  \"data/subdivisions/Analalava/ND analalava.shp\",\n  quiet = TRUE\n) |&gt;\n  st_transform(st_crs(wdpa_analalava))\n\n\ntm_shape(wdpa_analalava) +\n  tm_borders(col = \"red\", lwd = 2) +\n  tm_shape(sub_analalava_limite) +\n  tm_borders(col = \"blue\") +\n  tm_shape(sub_analalava_nd) +\n  tm_polygons(col = \"green\", fill = \"green\") +\n  tm_shape(sub_analalava_nd) +\n  tm_borders(col = \"purple\") +\n  tm_add_legend(\n    type = \"lines\",\n    labels = c(\"WDPA\", \"Limite\", \"ND\"),\n    col = c(\"red\", \"blue\", \"green\")\n  ) +\n  tm_add_legend(\n    type = \"fill\",\n    labels = c(\"ND\"),\n    col = c(\"lightgreen\")\n  )\n\n\n\n\n\n\n\n\nLes périmètres correspondent et les données semblent cohérentes.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#andrafiamena-andavakoera",
    "href": "07_subdivisions.html#andrafiamena-andavakoera",
    "title": "8  Validation des données de subdivision",
    "section": "8.8 Andrafiamena Andavakoera",
    "text": "8.8 Andrafiamena Andavakoera\nPlusieurs fichiers pour les données d’Andrafiamena\n\n\nCode\nlist.files(\n  \"data/subdivisions/Andrafiamena Andavakoera\",\n  pattern = \"\\\\.shp\"\n) |&gt;\n  cat(sep = \"\\n\")\n\n\nLimite AP Andrafiamena Andavakoera.shp\nNoyau dur AP Andrafiamena Andavakoera.shp\nZone de developpement agricole_ADF.shp\nZone de developpement du tourisme.shp\nZone de paturage_ADF.shp\nZone de reboisement_ADF.shp\n\n\n\n\nCode\nwdpa_andraf &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, regex(\"Andrafiamena\", ignore_case = TRUE)))\n\nsub_andraf_limite &lt;- st_read(\n  \"data/subdivisions/Andrafiamena Andavakoera/Limite AP Andrafiamena Andavakoera.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_andraf))\n\nsub_andraf_nd &lt;- st_read(\n  \"data/subdivisions/Andrafiamena Andavakoera/Noyau dur AP Andrafiamena Andavakoera.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_andraf))\n\nsub_andraf_dev_agri &lt;- st_read(\n  \"data/subdivisions/Andrafiamena Andavakoera/Zone de developpement agricole_ADF.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_andraf))\n\nsub_andraf_dev_tour &lt;- st_read(\n  \"data/subdivisions/Andrafiamena Andavakoera/Zone de developpement du tourisme.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_andraf))\n\nsub_andraf_paturage &lt;- st_read(\n  \"data/subdivisions/Andrafiamena Andavakoera/Zone de paturage_ADF.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_andraf))\n\nsub_andraf_rebois &lt;- st_read(\n  \"data/subdivisions/Andrafiamena Andavakoera/Zone de reboisement_ADF.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_andraf))\n\ntm_shape(wdpa_andraf) +\n  tm_borders(col = \"red\", lwd = 2) +\n  tm_shape(sub_andraf_limite) +\n  tm_borders(col = \"blue\") +\n  tm_shape(sub_andraf_nd) +\n  tm_polygons(col = \"green\", fill = \"green\") +\n  tm_shape(sub_andraf_dev_agri) +\n  tm_borders(col = \"brown\") +\n  tm_shape(sub_andraf_dev_tour) +\n  tm_borders(col = \"orange\") +\n  tm_shape(sub_andraf_paturage) +\n  tm_borders(col = \"purple\") +\n  tm_shape(sub_andraf_rebois) +\n  tm_borders(col = \"grey\") +\n  tm_add_legend(\n    type = \"lines\",\n    labels = c(\n      \"WDPA\",\n      \"Limite\",\n      \"ND\",\n      \"Dev agricole\",\n      \"Dev tourisme\",\n      \"Paturage\",\n      \"Reboisement\"\n    ),\n    col = c(\"red\", \"blue\", \"green\", \"brown\", \"orange\", \"purple\", \"grey\")\n  ) +\n  tm_add_legend(\n    type = \"fill\",\n    labels = c(\"ND\"),\n    col = c(\"lightgreen\")\n  )\n\n\n\n\n\n\n\n\nIl y a de très légères différences entre les périmètres externes WDPA et transmis MEDD. Les données de périmètres internes semblent riches et relativement cohérentes entre elles.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#anjozorobe-angavo",
    "href": "07_subdivisions.html#anjozorobe-angavo",
    "title": "8  Validation des données de subdivision",
    "section": "8.9 Anjozorobe Angavo",
    "text": "8.9 Anjozorobe Angavo\n\n\nCode\nlist.files(\n  \"data/subdivisions/Anjozorobe Angavo\",\n  pattern = \"\\\\.shp\"\n) |&gt;\n  cat(sep = \"\\n\")\n\n\nLimite AP Anjozorobe Angavo.shp\nNoyau dur AP Anjozorobe Angavo.shp\nZone Agroforesterie.shp\nZone de reboisement.shp\nZone de restauration.shp\nZone de suici ecologique.shp\nZone developpement agricole.shp\n\n\n\n\nCode\nwdpa_anjozorobe &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, regex(\"Anjozorobe\", ignore_case = TRUE))) |&gt;\n  filter(str_detect(NAME, regex(\"Angavo\", ignore_case = TRUE)))\n\nsub_anjo_limite &lt;- st_read(\n  \"data/subdivisions/Anjozorobe Angavo/Limite AP Anjozorobe Angavo.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_anjozorobe))\n\nsub_anjo_nd &lt;- st_read(\n  \"data/subdivisions/Anjozorobe Angavo/Noyau dur AP Anjozorobe Angavo.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_anjozorobe))\n\nsub_anjo_agro &lt;- st_read(\n  \"data/subdivisions/Anjozorobe Angavo/Zone Agroforesterie.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_anjozorobe))\n\nsub_anjo_rebois &lt;- st_read(\n  \"data/subdivisions/Anjozorobe Angavo/Zone de reboisement.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_anjozorobe))\n\nsub_anjo_restaur &lt;- st_read(\n  \"data/subdivisions/Anjozorobe Angavo/Zone de restauration.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_anjozorobe))\n\nsub_anjo_suivi &lt;- st_read(\n  \"data/subdivisions/Anjozorobe Angavo/Zone de suici ecologique.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_anjozorobe))\n\nsub_anjo_dev_agri &lt;- st_read(\n  \"data/subdivisions/Anjozorobe Angavo/Zone developpement agricole.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_anjozorobe))\n\ntm_shape(wdpa_anjozorobe) +\n  tm_borders(col = \"red\", lwd = 2) +\n  tm_shape(sub_anjo_limite) +\n  tm_borders(col = \"blue\") +\n  tm_shape(sub_anjo_nd) +\n  tm_polygons(col = \"green\", fill = \"green\") +\n  tm_shape(sub_anjo_agro) +\n  tm_borders(col = \"brown\") +\n  tm_shape(sub_anjo_rebois) +\n  tm_borders(col = \"orange\") +\n  tm_shape(sub_anjo_restaur) +\n  tm_borders(col = \"purple\") +\n  tm_shape(sub_anjo_suivi) +\n  tm_borders(col = \"grey\") +\n  tm_shape(sub_anjo_dev_agri) +\n  tm_borders(col = \"pink\") +\n  tm_add_legend(\n    type = \"lines\",\n    labels = c(\n      \"WDPA\",\n      \"Limite\",\n      \"ND\",\n      \"Agroforesterie\",\n      \"Reboisement\",\n      \"Restauration\",\n      \"Suivi ecologique\",\n      \"Dev agricole\"\n    ),\n    col = c(\"red\", \"blue\", \"green\", \"brown\", \"orange\", \"purple\", \"grey\", \"pink\")\n  ) +\n  tm_add_legend(\n    type = \"fill\",\n    labels = c(\"ND\"),\n    col = c(\"lightgreen\")\n  )\n\n\n\n\n\n\n\n\nLes périmètres externes fournis sont très proches de ceux de WDPA et les périmètres internes semblent cohérents.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#antrema",
    "href": "07_subdivisions.html#antrema",
    "title": "8  Validation des données de subdivision",
    "section": "8.10 Antrema",
    "text": "8.10 Antrema\n\n\nCode\nwdpa_antrema &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, \"Antrema\")) |&gt; # détecte l'AP et le site RAMSAR\n  filter(str_detect(DESIG, \"Ramsar\", negate = TRUE)) # on enlève le site Ramsar\n\nsub_antrema_limite &lt;- st_read(\n  \"data/subdivisions/Antrema/shapefiles/Zonage/Limite Antrema.shp\",\n  quiet = TRUE\n)\nsub_antrema_agropastorale &lt;- st_read(\n  \"data/subdivisions/Antrema/shapefiles/Zonage/Agropastorale.shp\",\n  quiet = TRUE\n)\nsub_antrema_recherche &lt;- st_read(\n  \"data/subdivisions/Antrema/shapefiles/Zonage/Recherche.shp\",\n  quiet = TRUE\n)\nsub_antrema_tourisme &lt;- st_read(\n  \"data/subdivisions/Antrema/shapefiles/Zonage/Ecotourisme.shp\",\n  quiet = TRUE\n)\nsub_antrema_noyau &lt;- st_read(\n  \"data/subdivisions/Antrema/shapefiles/Zonage/Noyau dur.shp\",\n  quiet = TRUE\n)\nsub_antrema_conservation &lt;- st_read(\n  \"data/subdivisions/Antrema/shapefiles/Zonage/Conservation.shp\",\n  quiet = TRUE\n)\nsub_antrema_reboisement &lt;- st_read(\n  \"data/subdivisions/Antrema/shapefiles/Zonage/Périmètre de reboisement.shp\",\n  quiet = TRUE\n)\nsub_antrema_marine &lt;- st_read(\n  \"data/subdivisions/Antrema/shapefiles/Zonage/Reserve marine.shp\",\n  quiet = TRUE\n)\nsub_antrema_utilisation_locale &lt;- st_read(\n  \"data/subdivisions/Antrema/shapefiles/Zonage/Utilisation locale.shp\",\n  quiet = TRUE\n)\n\ntm_shape(wdpa_antrema) +\n  tm_borders(col = \"red\") +\n  tm_shape(sub_antrema_limite) +\n  tm_borders(col = \"blue\") +\n  tm_shape(sub_antrema_agropastorale) +\n  tm_borders(col = \"purple\") +\n  tm_shape(sub_antrema_recherche) +\n  tm_borders(col = \"brown\") +\n  tm_shape(sub_antrema_tourisme) +\n  tm_borders(col = \"orange\") +\n  tm_shape(sub_antrema_noyau) +\n  tm_polygons(col = \"green\", fill = \"green\") +\n  tm_shape(sub_antrema_conservation) +\n  tm_borders(col = \"grey\") +\n  tm_shape(sub_antrema_reboisement) +\n  tm_borders(col = \"lightgreen\") +\n  tm_shape(sub_antrema_marine) +\n  tm_borders(col = \"darkblue\") +\n  tm_shape(sub_antrema_utilisation_locale) +\n  tm_borders(col = \"pink\") +\n  tm_add_legend(\n    type = \"lines\",\n    labels = c(\n      \"WDPA\",\n      \"Limite\",\n      \"Agropastorale\",\n      \"Recherche\",\n      \"Tourisme\",\n      \"Noyau dur\",\n      \"Conservation\",\n      \"Reboisement\",\n      \"Marine\",\n      \"Utilisation locale\"\n    ),\n    col = c(\n      \"red\",\n      \"blue\",\n      \"purple\",\n      \"brown\",\n      \"orange\",\n      \"green\",\n      \"grey\",\n      \"lightgreen\",\n      \"darkblue\",\n      \"pink\"\n    )\n  ) +\n  tm_add_legend(\n    type = \"fill\",\n    labels = c(\"ND\"),\n    col = c(\"lightgreen\")\n  )\n\n\n\n\n\n\n\n\nLe zonage pour Antrema semble particulièrement riche/complexe. On observe des décallages limités.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#beanka",
    "href": "07_subdivisions.html#beanka",
    "title": "8  Validation des données de subdivision",
    "section": "8.11 Beanka",
    "text": "8.11 Beanka\nLes fichiers pour Beanka n’ont pas été remis sous forme de shapefiles, mais sous forme de fichiers textes, avec des points codés en DMS.\n\n\nCode\nlist.files(\n  \"data/subdivisions/Beanka\"\n) |&gt;\n  cat(sep = \"\\n\")\n\n\nLimiteNAP_Beanka.txt\nLimiteNapBeanka.jpg\nnap_beankashp.apr\nNDurBeanka.txt\nNDureBeanka.jpg\n\n\nOn crée une procedure pour les géoréférencer proprement, on les convertis et on les visualise.\n\n\nCode\nparse_signed_dec &lt;- function(x) {\n  x &lt;- as.character(x)\n  x &lt;- str_replace_all(x, \"\\\\s+\", \"\")\n  sign &lt;- ifelse(str_detect(x, \"^[WwSs]\"), -1, 1)\n  val &lt;- readr::parse_number(x)\n  sign * val\n}\n\nmake_poly &lt;- function(df, lon, lat) {\n  xy &lt;- as.matrix(df[, c(lon, lat)])\n  xy &lt;- xy[stats::complete.cases(xy), , drop = FALSE]\n  if (nrow(xy) &lt; 3) {\n    return(NULL)\n  }\n  if (!all(xy[1, ] == xy[nrow(xy), ])) {\n    xy &lt;- rbind(xy, xy[1, ])\n  }\n  st_polygon(list(xy))\n}\n\nread_txt_polys_grouped &lt;- function(\n  file,\n  name_prefix,\n  lon_col,\n  lat_col,\n  group_col,\n  delim = \"\\t\",\n  encoding = \"Windows-1252\",\n  decimal_mark = \",\",\n  crs_out = 4326\n) {\n  dat &lt;- read_delim(\n    file,\n    delim = delim,\n    trim_ws = TRUE,\n    show_col_types = FALSE,\n    locale = locale(encoding = encoding, decimal_mark = decimal_mark)\n  )\n\n  dat2 &lt;- dat |&gt;\n    mutate(\n      lon = parse_signed_dec(.data[[lon_col]]),\n      lat = parse_signed_dec(.data[[lat_col]])\n    )\n\n  dat2 |&gt;\n    group_by(.data[[group_col]]) |&gt;\n    group_split() |&gt;\n    map(\\(df) {\n      p &lt;- make_poly(df, \"lon\", \"lat\")\n      if (is.null(p)) {\n        return(NULL)\n      }\n      st_sf(\n        NAME = paste0(name_prefix, \" \", as.character(df[[group_col]][1])),\n        Bloc = as.character(df[[group_col]][1]),\n        geometry = st_sfc(p, crs = crs_out)\n      )\n    }) |&gt;\n    compact() |&gt;\n    bind_rows() |&gt;\n    st_make_valid()\n}\n\nparse_dms_simple &lt;- function(x) {\n  hemi &lt;- str_sub(x, 1, 1)\n  body &lt;- str_trim(str_sub(x, 2))\n  parts &lt;- str_split_fixed(body, \" \", 2)\n  dm &lt;- parts[, 1]\n  sec &lt;- suppressWarnings(as.numeric(parts[, 2]))\n  dm_split &lt;- str_split_fixed(dm, \"[^0-9]+\", 2)\n  deg &lt;- suppressWarnings(as.numeric(dm_split[, 1]))\n  min &lt;- suppressWarnings(as.numeric(dm_split[, 2]))\n  dec &lt;- deg + min / 60 + sec / 3600\n  ifelse(hemi %in% c(\"W\", \"S\"), -dec, dec)\n}\n\nread_txt_poly_dms_cols &lt;- function(\n  file,\n  name,\n  lon_col,\n  lat_col,\n  delim = \"\\t\",\n  encoding = \"Windows-1252\",\n  decimal_mark = \",\"\n) {\n  dat &lt;- read_delim(\n    file,\n    delim = delim,\n    trim_ws = TRUE,\n    show_col_types = FALSE,\n    locale = locale(encoding = encoding, decimal_mark = decimal_mark)\n  )\n\n  pts &lt;- dat |&gt;\n    mutate(\n      lon = parse_dms_simple(.data[[lon_col]]),\n      lat = parse_dms_simple(.data[[lat_col]])\n    )\n\n  xy &lt;- as.matrix(pts[, c(\"lon\", \"lat\")])\n  xy &lt;- xy[stats::complete.cases(xy), , drop = FALSE]\n  if (!all(xy[1, ] == xy[nrow(xy), ])) {\n    xy &lt;- rbind(xy, xy[1, ])\n  }\n  st_sf(NAME = name, geometry = st_sfc(st_polygon(list(xy)), crs = 4326)) |&gt;\n    st_make_valid()\n}\n\nwdpa_beanka &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, regex(\"Beanka\", ignore_case = TRUE))) |&gt;\n  filter(!str_detect(DESIG, \"Ramsar\"))\n\nsub_beanka_nap &lt;- read_txt_poly_dms_cols(\n  file = \"data/subdivisions/Beanka/LimiteNAP_Beanka.txt\",\n  name = \"Limite NAP\",\n  lon_col = \"Longitude DMS\",\n  lat_col = \"Latitude DMS\"\n)\n\nsub_beanka_nd &lt;- read_txt_polys_grouped(\n  file = \"data/subdivisions/Beanka/NDurBeanka.txt\",\n  name_prefix = \"Noyau dur\",\n  lon_col = \"Long Dec\",\n  lat_col = \"Lat Dec\",\n  group_col = \"Bloc forestier\"\n)\n\ntm_shape(wdpa_beanka) +\n  tm_borders(col = \"red\", lwd = 2) +\n  tm_shape(sub_beanka_nap) +\n  tm_borders(col = \"blue\") +\n  tm_shape(sub_beanka_nd) +\n  tm_polygons(col = \"green\", fill = \"green\") +\n  tm_add_legend(\n    type = \"lines\",\n    labels = c(\"WDPA\", \"Limite (NAP)\"),\n    col = c(\"red\", \"blue\")\n  ) +\n  tm_add_legend(\n    type = \"fill\",\n    labels = c(\"ND\"),\n    col = c(\"lightgreen\")\n  )\n\n\n\n\n\n\n\n\nEn ce qui concerne les frontières de la NAP, les données fournies en .txt semblent concordantes avec celles de WDPA, hormis 2 points manquants et deux points comportant visiblement une erreur de saisie. Les données du noyau dur sont disponibles et sans anomalie apparente.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#behasina",
    "href": "07_subdivisions.html#behasina",
    "title": "8  Validation des données de subdivision",
    "section": "8.12 Behasina",
    "text": "8.12 Behasina\n\n\nCode\nlist.files(\n  \"data/subdivisions/Behasina\",\n  pattern = \"\\\\.shp\"\n) |&gt;\n  cat(sep = \"\\n\")\n\n\nbehasina 100.shp\nBehasina.shp\nNoyau dur Behasina.shp\nzuc behasina.shp\n\n\n\n\nCode\nwdpa_behasina &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, regex(\"Behasina\", ignore_case = TRUE)))\n\nsub_behasina_nd &lt;- st_read(\n  \"data/subdivisions/Behasina/Noyau dur Behasina.shp\",\n  quiet = TRUE\n) |&gt;\n  st_set_crs(29702) |&gt;\n  st_transform(st_crs(wdpa_behasina))\n\nsub_behasina_zuc &lt;- st_read(\n  \"data/subdivisions/Behasina/zuc behasina.shp\",\n  quiet = TRUE\n) |&gt;\n  st_transform(st_crs(wdpa_behasina))\n\nsub_behasina_limite &lt;- st_read(\n  \"data/subdivisions/Behasina/Behasina.shp\",\n  quiet = TRUE\n) |&gt; # Le SCR est manquant, on suppose que c'est le même que pour le ND\n  st_set_crs(29702) |&gt;\n  st_transform(st_crs(wdpa_behasina))\n\nsub_behasina_100 &lt;- st_read(\n  \"data/subdivisions/Behasina/behasina 100.shp\",\n  quiet = TRUE\n) |&gt; # Le SCR est manquant, on suppose que c'est le même que pour le ND\n  st_set_crs(st_crs(sub_behasina_nd)) |&gt;\n  st_transform(st_crs(wdpa_behasina))\n\ntm_shape(wdpa_behasina) +\n  tm_borders(col = \"red\", lwd = 2) +\n  tm_shape(sub_behasina_limite) +\n  tm_borders(col = \"blue\") +\n  tm_shape(sub_behasina_nd) +\n  tm_polygons(col = \"green\", fill = \"green\") +\n  tm_shape(sub_behasina_100) +\n  tm_borders(col = \"brown\") +\n  tm_shape(sub_behasina_zuc) +\n  tm_borders(col = \"orange\") +\n  tm_add_legend(\n    type = \"lines\",\n    labels = c(\"WDPA\", \"Limite\", \"ND\", \"Behasina 100\", \"ZUC\"),\n    col = c(\"red\", \"blue\", \"green\", \"brown\", \"orange\")\n  ) +\n  tm_add_legend(\n    type = \"fill\",\n    labels = c(\"ND\"),\n    col = c(\"lightgreen\")\n  )\n\n\n\n\n\n\n\n\nLes donnnées sont aberrante. Seule la ZUP semble avoir des données. Les autres sont vides ou aberrants.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#bemanevika",
    "href": "07_subdivisions.html#bemanevika",
    "title": "8  Validation des données de subdivision",
    "section": "8.13 Bemanevika",
    "text": "8.13 Bemanevika\n\n\nCode\nlist.files(\n  \"data/subdivisions/Bemanevika\",\n  pattern = \"\\\\.shp\"\n) |&gt;\n  cat(sep = \"\\n\")\n\n\nBemanevika_ProtectedArea.shp\n\n\nUn seul fichier ici.\n\n\nCode\nwdpa_bemanevika &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, regex(\"Bemanevika\", ignore_case = TRUE)))\n\nsub_bemanevika &lt;- st_read(\n  \"data/subdivisions/Bemanevika/Bemanevika_ProtectedArea.shp\",\n  quiet = TRUE,\n  options = \"ENCODING=Windows-1252\"\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_bemanevika))\n\n\ntm_shape(wdpa_bemanevika) +\n  tm_borders(col = \"red\", lwd = 2) +\n  tm_shape(sub_bemanevika) +\n  tm_polygons(\n    fill = \"SubZone\"\n  ) +\n  tm_borders(col = \"black\", lwd = 1)\n\n\n\n\n\n\n\nLe périmètre externe est proche de celui de WDPA et les périmètres internes semblent cohérents (SubZone == “Noyau Dur”).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#ivohiboro",
    "href": "07_subdivisions.html#ivohiboro",
    "title": "8  Validation des données de subdivision",
    "section": "8.14 Ivohiboro",
    "text": "8.14 Ivohiboro\n\n\nCode\nlist.files(\"data/subdivisions/Ivohiboro\") |&gt;\n  cat(sep = \"\\n\")\n\n\n95805\n\n\nLe document fourni n’a pas d’extension et ne contient pas de données lisibles.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#mahialambo",
    "href": "07_subdivisions.html#mahialambo",
    "title": "8  Validation des données de subdivision",
    "section": "8.15 Mahialambo",
    "text": "8.15 Mahialambo\n\n\nCode\nlist.files(\n  \"data/subdivisions/Mahialambo\",\n  pattern = \"\\\\.shp\"\n) |&gt;\n  cat(sep = \"\\n\")\n\n\nmahialambo 100.shp\nMahialambo.shp\nNoyau dur Mahialambo.shp\n\n\n\n\nCode\nwdpa_mahialambo &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, regex(\"Mahialambo\", ignore_case = TRUE)))\n\n\n# Assigner le CRS Laborde\nsub_mahialambo_limite &lt;- st_read(\n  \"data/subdivisions/Mahialambo/Mahialambo.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_set_crs(29702) |&gt; # LAborde\n  st_transform(st_crs(wdpa_mahialambo)) # Convertir en WGS84 pour affichage\n\nsub_mahialambo_100 &lt;- st_read(\n  \"data/subdivisions/Mahialambo/mahialambo 100.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_set_crs(29702) |&gt;\n  st_transform(st_crs(wdpa_mahialambo))\n\nsub_mahialambo_nd &lt;- st_read(\n  \"data/subdivisions/Mahialambo/Noyau dur Mahialambo.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_set_crs(29702) |&gt;\n  st_transform(st_crs(wdpa_mahialambo))\n\n\nRe-reading with feature count reset from 3 to 2\n\n\nCode\ntm_shape(wdpa_mahialambo) +\n  tm_borders(col = \"red\", lwd = 2) +\n  tm_shape(sub_mahialambo_limite) +\n  tm_borders(col = \"blue\") +\n  tm_shape(sub_mahialambo_nd) +\n  tm_polygons(fill = \"green\", fill_alpha = 0.6, col = \"darkgreen\") +\n  tm_shape(sub_mahialambo_100) +\n  tm_borders(col = \"purple\") +\n  tm_add_legend(\n    type = \"lines\",\n    labels = c(\"Limite\", \"ND\", \"Mahialambo 100\"),\n    col = c(\"blue\", \"green\", \"purple\")\n  ) +\n  tm_add_legend(\n    type = \"polygons\",\n    labels = c(\"ND\"),\n    fill = c(\"green\")\n  )\n\n\n\n\n\n\n\n\nLe SCR était manquant. On a pu deviner qu’il s’agissait de la projection laborde (EPSG: 29702). Une fois la bonne projection assignée, les frontières externes sont cohérentes avec celles de l’AP homonyme dans WDPA. Il doit toutefois manquer un point au tracé du Noyau Dur dans la partie Nord, car le ND dépasse de la limite externe du parc.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#mahimborondro",
    "href": "07_subdivisions.html#mahimborondro",
    "title": "8  Validation des données de subdivision",
    "section": "8.16 Mahimborondro",
    "text": "8.16 Mahimborondro\n\n\nCode\nlist.files(\n  \"data/subdivisions/Mahimborondro\",\n  pattern = \"\\\\.shp\"\n) |&gt;\n  cat(sep = \"\\n\")\n\n\nMahimborondro_ProtectedArea.shp\n\n\n\n\nCode\nwdpa_mahialambo &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, regex(\"Mahimborondro\", ignore_case = TRUE)))\n\nsub_mahimborondro &lt;- st_read(\n  \"data/subdivisions/Mahimborondro/Mahimborondro_ProtectedArea.shp\",\n  quiet = TRUE\n)\n\n\ntm_shape(wdpa_mahialambo) +\n  tm_borders(col = \"red\", lwd = 2) +\n  tm_shape(sub_mahimborondro) +\n  tm_borders(col = \"blue\") +\n  tm_add_legend(\n    type = \"lines\",\n    labels = c(\"WDPA\", \"Subdivisions\"),\n    col = c(\"red\", \"blue\")\n  )\n\n\n\n\n\n\n\nLes frontières externes correspondent à WDPA. Le fichier semble en effet contenir des subdivisions, mais on ne dispose que des géométries, sans attributs. Impossible de savoir par exemple laquelle (ou lesquelles) de ces subdivisions correspond au noyau dur.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#manambato",
    "href": "07_subdivisions.html#manambato",
    "title": "8  Validation des données de subdivision",
    "section": "8.17 Manambato",
    "text": "8.17 Manambato\n\n\nCode\nlist.files(\n  \"data/subdivisions/Manambato\",\n  pattern = \"\\\\.shp\"\n) |&gt;\n  cat(sep = \"\\n\")\n\n\nBloc Forestier Loky Manambato.shp\nLimite NAP Loky Manambato.shp\nLimite partie marine.shp\nNoyau dur Loky Manambato.shp\nNoyau dur partie marine.shp\nZone agropastorale.shp\nZone de restauration.shp\n\n\n\n\nCode\nwdpa_manambato &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, regex(\"Manambato|Loky\", ignore_case = TRUE)))\n\nsub_manambato_limite &lt;- st_read(\n  \"data/subdivisions/Manambato/Limite NAP Loky Manambato.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_manambato))\n\nsub_manambato_limite_marine &lt;- st_read(\n  \"data/subdivisions/Manambato/Limite partie marine.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_manambato))\n\nsub_manambato_nd &lt;- st_read(\n  \"data/subdivisions/Manambato/Noyau dur Loky Manambato.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_manambato))\n\nsub_manambato_nd_marine &lt;- st_read(\n  \"data/subdivisions/Manambato/Noyau dur partie marine.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_manambato))\n\nsub_manambato_bloc &lt;- st_read(\n  \"data/subdivisions/Manambato/Bloc Forestier Loky Manambato.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_manambato))\n\nsub_manambato_agro &lt;- st_read(\n  \"data/subdivisions/Manambato/Zone agropastorale.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_manambato))\n\nsub_manambato_restaur &lt;- st_read(\n  \"data/subdivisions/Manambato/Zone de restauration.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_manambato))\n\ntm_shape(wdpa_manambato) +\n  tm_borders(col = \"red\", lwd = 2) +\n  tm_shape(sub_manambato_limite) +\n  tm_borders(col = \"blue\") +\n  tm_shape(sub_manambato_limite_marine) +\n  tm_borders(col = \"blue\", lty = \"dashed\") +\n  tm_shape(sub_manambato_nd) +\n  tm_polygons(fill = \"green\", fill_alpha = 0.6, col = \"darkgreen\") +\n  tm_shape(sub_manambato_nd_marine) +\n  tm_polygons(fill = \"green\", fill_alpha = 0.6, col = \"darkgreen\") +\n  tm_shape(sub_manambato_bloc) +\n  tm_borders(col = \"brown\") +\n  tm_shape(sub_manambato_agro) +\n  tm_borders(col = \"orange\") +\n  tm_shape(sub_manambato_restaur) +\n  tm_borders(col = \"purple\") +\n  tm_add_legend(\n    type = \"lines\",\n    labels = c(\n      \"WDPA\",\n      \"Limite NAP (terrestre)\",\n      \"Limite NAP (marine)\",\n      \"Bloc forestier\",\n      \"Agropastorale\",\n      \"Restauration\"\n    ),\n    col = c(\"red\", \"blue\", \"blue\", \"brown\", \"orange\", \"purple\")\n  ) +\n  tm_add_legend(\n    type = \"polygons\",\n    labels = c(\"Noyau dur\"),\n    fill = c(\"green\")\n  )\n\n\n\n\n\n\n\n\nLes données semblent très cohérentes et alignées avec WDPA.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#mangoky-ihotry",
    "href": "07_subdivisions.html#mangoky-ihotry",
    "title": "8  Validation des données de subdivision",
    "section": "8.18 Mangoky Ihotry",
    "text": "8.18 Mangoky Ihotry\n\n\nCode\nlist.files(\n  \"data/subdivisions/Mangoky Ihotry\",\n  pattern = \"\\\\.shp\"\n) |&gt;\n  cat(sep = \"\\n\")\n\n\nLIMITE_CMI_FINAL.shp\nLIMITE_CMI_FINAL.shp.xml\nZONAGE_CMI_FINAL.shp\nZONAGE_CMI_FINAL.shp.xml\n\n\n\n\nCode\nwdpa_mangoky &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, regex(\"Mangoky|Ihotry|CMI\", ignore_case = TRUE)))\n\nsub_mangoky_limite &lt;- st_read(\n  \"data/subdivisions/Mangoky Ihotry/LIMITE_CMI_FINAL.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_mangoky))\n\nsub_mangoky_zonage &lt;- st_read(\n  \"data/subdivisions/Mangoky Ihotry/ZONAGE_CMI_FINAL.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_mangoky))\n\n\ntm_shape(sub_mangoky_limite) +\n  tm_borders(col = \"blue\") +\n  tm_shape(sub_mangoky_zonage) +\n  tm_polygons(fill = \"TYPE\") +\n  tm_shape(wdpa_mangoky) +\n  tm_borders(col = \"red\", lwd = 2) +\n  tm_add_legend(\n    type = \"lines\",\n    labels = c(\n      \"WDPA\"\n    ),\n    col = c(\"red\")\n  )\n\n\n\n\n\n\n\n\nOn observe un décalage à plusieurs endroits entre le périmètre de l’AP transmis et les limites WDPA.Le reste des données semble cohérent (les données de noyau dur son codées TYPE == “nd”).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#mandrozo",
    "href": "07_subdivisions.html#mandrozo",
    "title": "8  Validation des données de subdivision",
    "section": "8.19 Mandrozo",
    "text": "8.19 Mandrozo\n\n\nCode\nlist.files(\n  \"data/subdivisions/Mandrozo\",\n  pattern = \"\\\\.shp\"\n) |&gt;\n  cat(sep = \"\\n\")\n\n\nMandrozo_ProtectedArea.shp\n\n\n\n\nCode\nwdpa_mandrozo &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, regex(\"Mandrozo\", ignore_case = TRUE)))\n\nsub_mandrozo &lt;- st_read(\n  \"data/subdivisions/Mandrozo/Mandrozo_ProtectedArea.shp\",\n  quiet = TRUE,\n  options = \"ENCODING=Windows-1252\"\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_mandrozo))\n\ntm_shape(sub_mandrozo) +\n  tm_polygons(fill = \"ZONAGE\") +\n  tm_shape(wdpa_mandrozo) +\n  tm_borders(col = \"red\", lwd = 2) +\n  tm_add_legend(\n    type = \"lines\",\n    labels = c(\"WDPA\"),\n    col = c(\"red\")\n  )\n\n\n\n\n\n\n\n\nOn observe un léger décallage systématique dans le périmètre externe transmis et la donnée WDPA (problème de projection ?). Les données de noyau dur semblent convenir (codées ZONAGE == “Noyau Dur”)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#mangabe",
    "href": "07_subdivisions.html#mangabe",
    "title": "8  Validation des données de subdivision",
    "section": "8.20 Mangabe",
    "text": "8.20 Mangabe\n\n\nCode\nlist.files(\n  \"data/subdivisions/Mangabe\",\n  pattern = \"\\\\.shp\"\n) |&gt;\n  cat(sep = \"\\n\")\n\n\nLimite NAP Mangabe.shp\nLimite NAP Mangabe.shp.xml\n\n\n\n\nCode\nwdpa_mangabe &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, regex(\"Mangabe\", ignore_case = TRUE)))\n\nsub_mangabe_limite &lt;- st_read(\n  \"data/subdivisions/Mangabe/Limite NAP Mangabe.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_mangabe))\n\ntm_shape(wdpa_mangabe) +\n  tm_borders(col = \"red\", lwd = 2) +\n  tm_shape(sub_mangabe_limite) +\n  tm_borders(col = \"blue\") +\n  tm_add_legend(\n    type = \"lines\",\n    labels = c(\"WDPA\", \"Limite NAP\"),\n    col = c(\"red\", \"blue\")\n  )\n\n\n\n\n\n\n\nLa donnée transmise est cohérente avec WDPA, mais elle ne contient que le périmètre extérieur : pas de subdivisions.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#menabe-antimena",
    "href": "07_subdivisions.html#menabe-antimena",
    "title": "8  Validation des données de subdivision",
    "section": "8.21 Menabe Antimena",
    "text": "8.21 Menabe Antimena\n\n\nCode\nlist.files(\n  \"data/subdivisions/Menabe Antimena\",\n  pattern = \"\\\\.shp\"\n) |&gt;\n  cat(sep = \"\\n\")\n\n\nceinture_verte_kimanomby.shp\nceinture_verte_noyau_dur.shp\nnd_apma.shp\nzone_d_occupation_controllee.shp\nzone_d_utilisation_durable.shp\nzone_de_culture.shp\nzone_de_paturage.shp\nzone_de_prelevement_durable.shp\nzone_de_reboisement.shp\nzone_de_restauration_dans_noyau_dur.shp\nzone_de_restauration_des_sols.shp\nzone_de_restauration_hors_noyau_dur.shp\nzone_de_service.shp\nzone_forestiere_dans_noyau_dur.shp\nzone_littorale.shp\nzone_tampon.shp\n\n\n\n\nCode\nwdpa_menabe &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, regex(\"Menabe|Antimena\", ignore_case = TRUE)))\n\nsub_menabe_nd &lt;- st_read(\n  \"data/subdivisions/Menabe Antimena/nd_apma.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_menabe))\n\nsub_menabe_zoc &lt;- st_read(\n  \"data/subdivisions/Menabe Antimena/zone_d_occupation_controllee.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_menabe))\n\nsub_menabe_zud &lt;- st_read(\n  \"data/subdivisions/Menabe Antimena/zone_d_utilisation_durable.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_menabe))\n\nsub_menabe_culture &lt;- st_read(\n  \"data/subdivisions/Menabe Antimena/zone_de_culture.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_menabe))\n\nsub_menabe_paturage &lt;- st_read(\n  \"data/subdivisions/Menabe Antimena/zone_de_paturage.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_menabe))\n\nsub_menabe_prelev &lt;- st_read(\n  \"data/subdivisions/Menabe Antimena/zone_de_prelevement_durable.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_menabe))\n\nsub_menabe_rebois &lt;- st_read(\n  \"data/subdivisions/Menabe Antimena/zone_de_reboisement.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_menabe))\n\nsub_menabe_service &lt;- st_read(\n  \"data/subdivisions/Menabe Antimena/zone_de_service.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_menabe))\n\nsub_menabe_littorale &lt;- st_read(\n  \"data/subdivisions/Menabe Antimena/zone_littorale.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_menabe))\n\nsub_menabe_tampon &lt;- st_read(\n  \"data/subdivisions/Menabe Antimena/zone_tampon.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_menabe))\n\nsub_menabe_rest_in_nd &lt;- st_read(\n  \"data/subdivisions/Menabe Antimena/zone_de_restauration_dans_noyau_dur.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_menabe))\n\nsub_menabe_rest_hors_nd &lt;- st_read(\n  \"data/subdivisions/Menabe Antimena/zone_de_restauration_hors_noyau_dur.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_menabe))\n\nsub_menabe_rest_sols &lt;- st_read(\n  \"data/subdivisions/Menabe Antimena/zone_de_restauration_des_sols.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_menabe))\n\nsub_menabe_forest_in_nd &lt;- st_read(\n  \"data/subdivisions/Menabe Antimena/zone_forestiere_dans_noyau_dur.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_menabe))\n\nsub_menabe_cv_kimanomby &lt;- st_read(\n  \"data/subdivisions/Menabe Antimena/ceinture_verte_kimanomby.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_menabe))\n\nsub_menabe_cv_nd &lt;- st_read(\n  \"data/subdivisions/Menabe Antimena/ceinture_verte_noyau_dur.shp\",\n  quiet = TRUE\n) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_menabe))\n\ntm_shape(wdpa_menabe) +\n  tm_borders(col = \"red\", lwd = 2) +\n  tm_shape(sub_menabe_nd) +\n  tm_polygons(fill = \"green\", fill_alpha = 0.6, col = \"darkgreen\") +\n  tm_shape(sub_menabe_zoc) +\n  tm_borders(col = \"purple\") +\n  tm_shape(sub_menabe_zud) +\n  tm_borders(col = \"orange\") +\n  tm_shape(sub_menabe_tampon) +\n  tm_borders(col = \"blue\") +\n  tm_shape(sub_menabe_littorale) +\n  tm_borders(col = \"brown\") +\n  tm_shape(sub_menabe_culture) +\n  tm_borders(col = \"grey\") +\n  tm_shape(sub_menabe_paturage) +\n  tm_borders(col = \"black\") +\n  tm_shape(sub_menabe_prelev) +\n  tm_borders(col = \"pink\") +\n  tm_shape(sub_menabe_rebois) +\n  tm_borders(col = \"lightgreen\") +\n  tm_shape(sub_menabe_service) +\n  tm_borders(col = \"yellow\") +\n  tm_shape(sub_menabe_rest_in_nd) +\n  tm_borders(col = \"darkblue\") +\n  tm_shape(sub_menabe_rest_hors_nd) +\n  tm_borders(col = \"darkred\") +\n  tm_shape(sub_menabe_rest_sols) +\n  tm_borders(col = \"cyan\") +\n  tm_shape(sub_menabe_forest_in_nd) +\n  tm_borders(col = \"darkgrey\") +\n  tm_shape(sub_menabe_cv_kimanomby) +\n  tm_borders(col = \"gold\") +\n  tm_shape(sub_menabe_cv_nd) +\n  tm_borders(col = \"goldenrod\") +\n  tm_add_legend(\n    type = \"lines\",\n    labels = c(\n      \"WDPA\",\n      \"ZOC\",\n      \"ZUD\",\n      \"Zone tampon\",\n      \"Zone littorale\",\n      \"Zone de culture\",\n      \"Zone de pâturage\",\n      \"Prélèvement durable\",\n      \"Reboisement\",\n      \"Zone de service\",\n      \"Restauration dans ND\",\n      \"Restauration hors ND\",\n      \"Restauration des sols\",\n      \"Zone forestière dans ND\",\n      \"Ceinture verte Kimanomby\",\n      \"Ceinture verte ND\"\n    ),\n    col = c(\n      \"red\",\n      \"purple\",\n      \"orange\",\n      \"blue\",\n      \"brown\",\n      \"grey\",\n      \"black\",\n      \"pink\",\n      \"lightgreen\",\n      \"yellow\",\n      \"darkblue\",\n      \"darkred\",\n      \"cyan\",\n      \"darkgrey\",\n      \"gold\",\n      \"goldenrod\"\n    )\n  ) +\n  tm_add_legend(\n    type = \"polygons\",\n    labels = c(\"Noyau dur\"),\n    fill = c(\"green\")\n  )\n\n\n\n\n\n\n\n\nLe périmètre externe est parfaitement cohérent avec WDPA. La donné est particulièrement détaillée.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#sahafina",
    "href": "07_subdivisions.html#sahafina",
    "title": "8  Validation des données de subdivision",
    "section": "8.22 Sahafina",
    "text": "8.22 Sahafina\n\n\nCode\nlist.files(\n  \"data/subdivisions/Sahafina\"\n) |&gt;\n  cat(sep = \"\\n\")\n\n\nDelimNapSahafina.jpg\nLimiteSahafina.txt\nnoyaudurpolygoneBeanka.shp\nnoyaudurpolygoneSahafina.shp\nnoyaudurpolygoneSahafina.shx\nsahafinashp.dbf\nsahafinashp.shp\nsahafinashp.shx\nSchemaNDurSahafina.jpg\n\n\n\n\nCode\nwdpa_sahafina &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, regex(\"Sahafina\", ignore_case = TRUE)))\n\nsub_sahafina_limite_shp &lt;- st_read(\n  \"data/subdivisions/Sahafina/sahafinashp.shp\",\n  quiet = TRUE\n) |&gt;\n  st_set_crs(29702) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_sahafina))\n\n# On essaye de reconstruire le fichier shx manquant \"en vol\"\nSys.setenv(SHAPE_RESTORE_SHX = \"YES\")\nnd_path &lt;- \"data/subdivisions/Sahafina/noyaudurpolygoneSahafina.shp\"\n\nsub_sahafina_nd &lt;- st_read(nd_path, quiet = TRUE) |&gt;\n  st_set_crs(29702) |&gt;\n  st_make_valid()\n\ntm_shape(wdpa_sahafina) +\n  tm_borders(col = \"red\", lwd = 2) +\n  tm_shape(sub_sahafina_limite_shp) +\n  tm_borders(col = \"blue\") +\n  tm_shape(sub_sahafina_nd) +\n  tm_polygons(fill = \"green\", fill_alpha = 0.6, col = \"darkgreen\") +\n  tm_add_legend(\n    type = \"lines\",\n    labels = c(\"WDPA\", \"Limite (shp)\"),\n    col = c(\"red\", \"blue\")\n  ) +\n  tm_add_legend(\n    type = \"polygons\",\n    labels = c(\"Noyau dur\"),\n    fill = c(\"green\")\n  )\n\n\n\n\n\n\n\n\nLes données semblent correct: le périmètre externe est bien aligné avec celui de WDPA et le périmètre interne est cohérent.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#tsimembo",
    "href": "07_subdivisions.html#tsimembo",
    "title": "8  Validation des données de subdivision",
    "section": "8.23 Tsimembo",
    "text": "8.23 Tsimembo\n\n\nCode\nlist.files(\n  \"data/subdivisions/Tsimembo\",\n  pattern = \"\\\\.shp\"\n) |&gt;\n  cat(sep = \"\\n\")\n\n\nTsimemboManambolomaty_ProtectedArea.shp\n\n\n\n\nCode\nwdpa_tsimembo &lt;- wdpa_mdg |&gt;\n  filter(str_detect(NAME, regex(\"Tsimembo|Manambolomaty\", ignore_case = TRUE)))\n\nsub_tsimembo &lt;- st_read(\n  \"data/subdivisions/Tsimembo/TsimemboManambolomaty_ProtectedArea.shp\",\n  quiet = TRUE\n) |&gt;\n  st_set_crs(29702) |&gt;\n  st_make_valid() |&gt;\n  st_transform(st_crs(wdpa_tsimembo))\n\ntm_shape(sub_tsimembo) +\n  tm_polygons(fill = \"SubZone\") +\n  tm_shape(wdpa_tsimembo) +\n  tm_borders(col = \"red\", lwd = 2) +\n  tm_add_legend(\n    type = \"lines\",\n    labels = c(\"WDPA\", \"Subdivision\"),\n    col = c(\"red\", \"blue\")\n  )\n\n\n\n\n\n\n\n\nOn observe un léger décallage systématique des données (problème de projection)\nLes données semblent cohérentes. Le Noyau dur est codé : SubZone == “Noyau Dur”.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "07_subdivisions.html#on-exporte-finalement-les-noyaux-durs-exploitables-pour-utilisation-ultérieure",
    "href": "07_subdivisions.html#on-exporte-finalement-les-noyaux-durs-exploitables-pour-utilisation-ultérieure",
    "title": "8  Validation des données de subdivision",
    "section": "9.1 On exporte finalement les noyaux durs exploitables pour utilisation ultérieure",
    "text": "9.1 On exporte finalement les noyaux durs exploitables pour utilisation ultérieure\n\n\nCode\nlibrary(lwgeom)\n\nextract_wdpa_id_name &lt;- function(wdpa_sf) {\n  x &lt;- wdpa_sf |&gt;\n    st_drop_geometry() |&gt;\n    select(WDPAID, NAME)\n\n  tibble(\n    WDPAID = suppressWarnings(as.numeric(x$WDPAID[1])),\n    NAME = as.character(x$NAME[1])\n  )\n}\n\nnd_to_multipolygon &lt;- function(\n  nd_sf,\n  crs_target,\n  crs_work = 29702, # Laborde Madagascar, adapté pour l'union\n  snap = 1e-3 # en mètres dans crs_work, ajuste si besoin\n) {\n  x &lt;- nd_sf |&gt;\n    st_transform(crs_target) |&gt;\n    sf::st_make_valid()\n\n  # Union en CRS projeté, plus robuste\n  x &lt;- x |&gt;\n    st_transform(crs_work) |&gt;\n    sf::st_make_valid()\n\n  # Option \"snap to grid\" pour casser certaines auto-intersections numériques\n  # (snap en mètres, ici 1 mm)\n  x &lt;- st_set_precision(x, 1 / snap) |&gt;\n    st_snap_to_grid(snap)\n\n  u &lt;- tryCatch(\n    st_union(x),\n    error = function(e) NULL\n  )\n\n  # Secours: buffer 0 (en CRS projeté) puis union\n  if (is.null(u)) {\n    x2 &lt;- tryCatch(st_buffer(x, 0), error = function(e) x)\n    u &lt;- st_union(x2)\n  }\n\n  u &lt;- u |&gt;\n    sf::st_make_valid() |&gt;\n    st_collection_extract(\"POLYGON\", warn = FALSE) |&gt;\n    st_cast(\"MULTIPOLYGON\", warn = FALSE) |&gt;\n    st_transform(crs_target) |&gt;\n    sf::st_make_valid()\n\n  u\n}\n\n\nmake_nd_row &lt;- function(pa_label, wdpa_sf, nd_sf) {\n  idn &lt;- extract_wdpa_id_name(wdpa_sf)\n  geom &lt;- nd_to_multipolygon(nd_sf, crs_target = st_crs(wdpa_sf))\n\n  st_sf(\n    pa_label = pa_label,\n    WDPAID = idn$WDPAID,\n    NAME = idn$NAME,\n    geometry = geom,\n    crs = st_crs(wdpa_sf)\n  )\n}\n\nnd_rows &lt;- list(\n  make_nd_row(\"Allée des Baobabs\", wdpa_baobabs, sub_baobabs_noyau_dur),\n  make_nd_row(\"Ambatofotsy\", wdpa_ambatofotsy, sub_ambatofotsy_nd),\n  make_nd_row(\"Ampotaka Ankorabe\", wdpa_ampotaka, sub_ampotaka_nd),\n  make_nd_row(\"Analabe Betanantanana\", wdpa_analabe, sub_analabe_nd),\n  make_nd_row(\"Analalava\", wdpa_analalava, sub_analalava_nd),\n  make_nd_row(\"Andrafiamena Andavakoera\", wdpa_andraf, sub_andraf_nd),\n  make_nd_row(\"Anjozorobe Angavo\", wdpa_anjozorobe, sub_anjo_nd),\n  make_nd_row(\"Antrema\", wdpa_antrema, sub_antrema_noyau),\n\n  make_nd_row(\"Beanka\", wdpa_beanka, sub_beanka_nd),\n\n  make_nd_row(\n    \"Bemanevika\",\n    wdpa_bemanevika,\n    sub_bemanevika |&gt;\n      filter(str_detect(\n        SubZone,\n        regex(\"^\\\\s*Noyau\\\\s*Dur\\\\s*$\", ignore_case = TRUE)\n      ))\n  ),\n\n  make_nd_row(\"Mahialambo\", wdpa_mahialambo, sub_mahialambo_nd),\n\n  make_nd_row(\n    \"Mangoky Ihotry\",\n    wdpa_mangoky,\n    sub_mangoky_zonage |&gt;\n      filter(str_detect(TYPE, regex(\"^\\\\s*nd\\\\s*$\", ignore_case = TRUE)))\n  ),\n\n  make_nd_row(\n    \"Mandrozo\",\n    wdpa_mandrozo,\n    sub_mandrozo |&gt;\n      filter(str_detect(ZONAGE, regex(\"Noyau\\\\s*Dur\", ignore_case = TRUE)))\n  ),\n\n  make_nd_row(\n    \"Manambato\",\n    wdpa_manambato,\n    bind_rows(sub_manambato_nd, sub_manambato_nd_marine)\n  ),\n\n  make_nd_row(\"Menabe Antimena\", wdpa_menabe, sub_menabe_nd),\n\n  make_nd_row(\"Sahafina\", wdpa_sahafina, sub_sahafina_nd),\n\n  make_nd_row(\n    \"Tsimembo\",\n    wdpa_tsimembo,\n    sub_tsimembo |&gt;\n      filter(str_detect(SubZone, regex(\"Noyau\\\\s*Dur\", ignore_case = TRUE)))\n  )\n)\n\nnd_sf &lt;- nd_rows |&gt;\n  bind_rows() |&gt;\n  st_make_valid() |&gt;\n  mutate(\n    WDPAID = as.numeric(WDPAID),\n    NAME = as.character(NAME)\n  )\n\nst_write(\n  nd_sf,\n  \"data/noyaux_durs_exploitables.geojson\",\n  driver = \"GeoJSON\",\n  delete_dsn = TRUE,\n  quiet = TRUE\n)\n\n\nLes données contenant les périmètres internes des 17 AP exploitables sont maintenant enregistrées.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Validation des données de subdivision</span>"
    ]
  },
  {
    "objectID": "08_border_changes.html",
    "href": "08_border_changes.html",
    "title": "9  Changement de frontières",
    "section": "",
    "text": "9.1 Revue des principales modifications\nOn charge les données de zones protégées historiques transmises par MNP et on les compare à la version de Novembre 2025 de WDPA.\nOn va maintenant retrouver, pour chaque polygone de\nOn voit qu’on a plusieurs polygones historiques transmis par MNP qui appartiennent à une même aire protégée dans la version WDPA de Novembre 2025. On va donc les regrouper dans une même entité, afin d’avoir, pour chaque aire protégée, d’un côté une entité rassemblant les polygones historiques et de l’autre côté le polygone WDPA actuel.\nMaintenant on va comparer leur similarité et repéré les 15 plus différents.\nCode\n# Equal area CRS for area ratios\nea_crs &lt;- 29702 # Laborde\n\n# WDPA as multipolygon by WDPAID (recommended)\nwdpa_by_id &lt;- wdpa_mdg %&gt;%\n  select(WDPAID, WDPA_NAME = NAME) %&gt;%\n  st_make_valid() %&gt;%\n  group_by(WDPAID, WDPA_NAME) %&gt;%\n  summarise(geometry = st_union(geometry), .groups = \"drop\") %&gt;%\n  st_collection_extract(\"POLYGON\") %&gt;%\n  st_cast(\"MULTIPOLYGON\")\n\n# Transform + areas\nhist_ea &lt;- mnp_hist_by_wdpaid %&gt;%\n  st_make_valid() %&gt;%\n  st_transform(ea_crs) %&gt;%\n  mutate(hist_area = as.numeric(st_area(geometry)), hist_geom = geometry) %&gt;%\n  st_drop_geometry() %&gt;%\n  rename(WDPA_NAME_hist = WDPA_NAME)\n\nwdpa_ea &lt;- wdpa_by_id %&gt;%\n  st_make_valid() %&gt;%\n  st_transform(ea_crs) %&gt;%\n  mutate(wdpa_area = as.numeric(st_area(geometry)), wdpa_geom = geometry) %&gt;%\n  st_drop_geometry()\n\ncomp &lt;- hist_ea %&gt;%\n  left_join(wdpa_ea, by = \"WDPAID\") %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    inter_area = if (is.na(wdpa_area)) {\n      0\n    } else {\n      as.numeric(sum(st_area(st_intersection(hist_geom, wdpa_geom))))\n    },\n    pct_hist_within_wdpa = ifelse(\n      hist_area &gt; 0,\n      inter_area / hist_area,\n      NA_real_\n    ),\n    pct_wdpa_within_hist = ifelse(\n      !is.na(wdpa_area) & wdpa_area &gt; 0,\n      inter_area / wdpa_area,\n      NA_real_\n    ),\n    similarity = pmin(pct_hist_within_wdpa, pct_wdpa_within_hist, na.rm = TRUE),\n    dissimilarity = 1 - similarity\n  ) %&gt;%\n  ungroup() %&gt;%\n  select(\n    WDPAID,\n    WDPA_NAME_hist,\n    WDPA_NAME,\n    pct_hist_within_wdpa,\n    pct_wdpa_within_hist,\n    similarity,\n    dissimilarity\n  ) %&gt;%\n  arrange(desc(dissimilarity))\n\ncomp %&gt;%\n  slice_head(n = 15) %&gt;%\n  mutate(\n    pct_hist_within_wdpa = scales::percent(\n      pct_hist_within_wdpa,\n      accuracy = 0.1\n    ),\n    pct_wdpa_within_hist = scales::percent(\n      pct_wdpa_within_hist,\n      accuracy = 0.1\n    ),\n    similarity = round(similarity, 3)\n  ) %&gt;%\n  select(\n    WDPA_NAME,\n    pct_hist_within_wdpa,\n    pct_wdpa_within_hist,\n    similarity\n  ) %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Comparaison des limites historiques et WDPA\",\n    subtitle = \"15 aires protégées les plus dissemblables\"\n  ) %&gt;%\n  cols_label(\n    WDPA_NAME = \"Aire protégée\",\n    pct_hist_within_wdpa = \"Part du périmètre historique incluse dans WDPA\",\n    pct_wdpa_within_hist = \"Part de WDPA incluse dans le périmètre historique\",\n    similarity = \"Indice de similarité\"\n  ) %&gt;%\n  fmt_number(columns = similarity, decimals = 3) %&gt;%\n  data_color(\n    columns = similarity,\n    domain = c(0, 1),\n    palette = c(\"#d73027\", \"#fee08b\", \"#1a9850\")\n  ) %&gt;%\n  opt_table_outline()\n\n\n\n\n\n\n\n\nComparaison des limites historiques et WDPA\n\n\n15 aires protégées les plus dissemblables\n\n\nAire protégée\nPart du périmètre historique incluse dans WDPA\nPart de WDPA incluse dans le périmètre historique\nIndice de similarité\n\n\n\n\nBaie d'Antogil\n100.0%\n0.3%\n0.003\n\n\nComplexe des AP Ambohimirahavavy Marivorahona\n100.0%\n8.9%\n0.089\n\n\nBeza Mahafaly\n99.5%\n11.4%\n0.114\n\n\nTsimanampesotse\n99.4%\n23.2%\n0.232\n\n\nZones Humides de Sahamalaza\n95.4%\n46.9%\n0.469\n\n\nKirindy Mite\n96.3%\n48.9%\n0.489\n\n\nCap Sainte Marie\n98.4%\n61.4%\n0.614\n\n\nAnjanaharibe_sud\n97.9%\n63.5%\n0.635\n\n\nManongarivo\n99.1%\n80.5%\n0.805\n\n\nAmbatovaky\n82.6%\n80.9%\n0.809\n\n\nBefotaka Midongy\n88.7%\n100.0%\n0.887\n\n\nMananara Nord\n90.7%\n94.8%\n0.907\n\n\nMontagne d'Ambre\n97.1%\n90.8%\n0.908\n\n\nMasoala\n92.9%\n95.2%\n0.929\n\n\nRanomafana\n93.4%\n100.0%\n0.934\nMaintenant, on visualise sous forme de carte simple.\nCode\nplot_comp_facets &lt;- function(comp, rows, hist_sf, wdpa_sf, ncol = 1) {\n  sub &lt;- dplyr::slice(comp, rows) %&gt;%\n    dplyr::transmute(WDPAID, panel_name = WDPA_NAME) %&gt;%\n    dplyr::filter(!is.na(WDPAID))\n\n  hist &lt;- hist_sf %&gt;%\n    dplyr::semi_join(sub, by = \"WDPAID\") %&gt;%\n    dplyr::select(WDPAID, geometry) %&gt;%\n    dplyr::mutate(layer = \"Historical\")\n\n  wdpa &lt;- wdpa_sf %&gt;%\n    dplyr::semi_join(sub, by = \"WDPAID\") %&gt;%\n    dplyr::select(WDPAID, geometry) %&gt;%\n    dplyr::mutate(layer = \"WDPA\")\n\n  dat &lt;- dplyr::bind_rows(wdpa, hist) %&gt;%\n    dplyr::left_join(sub, by = \"WDPAID\") %&gt;%\n    dplyr::mutate(\n      WDPAID = factor(WDPAID, levels = sub$WDPAID),\n      layer = factor(layer, levels = c(\"WDPA\", \"Historical\"))\n    )\n\n  tmap::tmap_mode(\"plot\")\n\n  tmap::tm_shape(dat) +\n    tmap::tm_polygons(\n      fill = \"layer\",\n      fill.scale = tmap::tm_scale(\n        values = c(WDPA = \"blue\", Historical = \"red\")\n      ),\n      fill_alpha = 0.5,\n      col = NA\n    ) +\n    tmap::tm_facets(by = \"WDPAID\", ncol = ncol) +\n    tmap::tm_layout(\n      legend.outside = TRUE,\n      panel.labels = sub$panel_name\n    )\n}\n\n# example\nplot_comp_facets(\n  comp,\n  rows = 1:15,\n  hist_sf = mnp_hist_by_wdpaid,\n  wdpa_sf = wdpa_by_id,\n  ncol = 3\n)\nCode\nlegal_texts.rds &lt;- read_rds(\"data/id/legal_texts.rds\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Changement de frontières</span>"
    ]
  },
  {
    "objectID": "08_border_changes.html#questions-en-suspens",
    "href": "08_border_changes.html#questions-en-suspens",
    "title": "9  Changement de frontières",
    "section": "9.2 Questions en suspens",
    "text": "9.2 Questions en suspens\n\nValidation de ces différences\nQuelles dates d’entrée en vigueur des modifications de limites ?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Changement de frontières</span>"
    ]
  },
  {
    "objectID": "09fr_technical_proposal.html",
    "href": "09fr_technical_proposal.html",
    "title": "10  Proposition technique",
    "section": "",
    "text": "Le jeu de données est conçu comme une extension explicitement temporelle des polygones de la WDPA, intégrant, lorsque ces informations sont disponibles, l’historique juridique, les évolutions spatiales et le zonage interne des aires protégées. La WDPA fournit l’état courant par défaut de chaque aire protégée, mais cet état est considéré comme un seul segment au sein d’une trajectoire plus longue, marquée par des changements de statut juridique, de périmètre et d’organisation interne. Pour chaque WDPAID, le jeu de données reconstruit ainsi des états successifs de l’aire protégée, chacun défini par une période de validité explicite, en mobilisant des informations juridiques et spatiales datées lorsqu’elles existent, tout en restant pleinement compatible avec la WDPA en l’absence de telles informations.\nLe principe central repose sur un remplissage par défaut combiné à des règles de priorité explicites et déterministes. En l’absence d’informations alternatives, les attributs et la géométrie de la WDPA définissent un état unique, dont la période de validité débute à partir de STATUS_YR, interprétée selon une convention de datation documentée, et s’étend jusqu’à la date de référence de la version de la WDPA utilisée. Lorsque des décisions juridiques antérieures, des périmètres plus anciens ou des dispositifs de zonage interne sont identifiés à partir de sources complémentaires, la chronologie est découpée en plusieurs intervalles de validité non chevauchants. Les états antérieurs héritent par défaut des attributs de la WDPA, sauf lorsque les textes juridiques, les périmètres datés ou les documents de zonage indiquent explicitement des différences, telles qu’un statut de protection temporaire, des limites alternatives, des changements de gestion ou une organisation interne spécifique.\nL’historique juridique, l’évolution des périmètres et la mise en place du zonage interne sont traités comme des sources indépendantes de segmentation temporelle. Les dates issues des textes juridiques, des instruments de zonage ou des modifications de limites structurent conjointement la chronologie. Chaque intervalle ainsi défini est ensuite renseigné avec l’information la plus précise disponible pour la période considérée, tant au niveau de l’aire protégée que, le cas échéant, de ses zones internes. Les limites externes sont traitées comme un type spécifique de zone, ce qui permet d’appliquer une logique spatiale unifiée aux périmètres et au zonage interne.\nConcrètement, le jeu de données ne remplace pas la WDPA. Il est construit à partir de quatre composantes complémentaires, chacune ayant un rôle clairement défini :\nWDPA\nLe jeu de données mondial de polygones de la WDPA, utilisé comme référence de base décrivant l’état courant des aires protégées.\nSAT – Spatial Amendment Table\nUne table d’amendements spatiaux contenant des périmètres historiques datés et des géométries de zonage interne qui diffèrent de la représentation actuelle de la WDPA.\nFAT – Feature Amendment Table\nUne table d’amendements attributaires contenant des modifications juridiques et administratives datées, telles que des changements de statut de protection, des délégations de gestion, des désignations ou des modalités de gouvernance.\nCAR – Consolidation and Auditing Rules\nUn ensemble stable et transparent de règles de consolidation, exprimées sous forme de code lisible par des humains et reproductible, qui précisent comment la WDPA, la SAT et la FAT sont combinées, comment la segmentation temporelle est effectuée, et comment les règles de priorité entre sources sont appliquées et rendues auditables.\nPris ensemble, ces éléments permettent de produire une table d’états dynamiques dérivée, dans laquelle chaque aire protégée de la WDPA est représentée par une succession d’états délimités dans le temps. La WDPA fournit l’état courant par défaut, tandis que la SAT et la FAT n’interviennent que lorsque des informations datées explicites sont disponibles, entraînant un découpage de la chronologie et, le cas échéant, une modification de la géométrie, du statut juridique ou du zonage interne, conformément aux règles définies par la CAR. En l’absence de telles informations, les données de la WDPA sont simplement prolongées vers le passé ou vers le présent sans modification, selon des conventions documentées.\nLe jeu de données résultant demeure pleinement compatible avec les identifiants et la structure de la WDPA, tout en rendant explicites, reproductibles et vérifiables les hypothèses temporelles sous-jacentes. La contribution réside ainsi à la fois dans la publication de données complémentaires soigneusement construites sur les trajectoires juridiques et spatiales (SAT et FAT), et dans les procédures transparentes de consolidation et d’audit (CAR) qui permettent de transformer la WDPA en un jeu de données de conservation explicitement temporel, adapté aux analyses longitudinales et aux évaluations d’impact.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Proposition technique</span>"
    ]
  },
  {
    "objectID": "09en_technical_proposal.html",
    "href": "09en_technical_proposal.html",
    "title": "11  Technical proposal (English)",
    "section": "",
    "text": "The dataset is designed as a time explicit extension of WDPA polygon data that incorporates legal history, spatial changes, and internal zoning of protected areas when such information is available. WDPA provides the default current state of each protected area, but this state is treated as a single segment within a longer trajectory marked by changes in legal status, boundaries, and internal organization. For each WDPAID, the dataset reconstructs successive states of the protected area, each defined by explicit validity periods, by mobilizing dated legal and spatial information when available, while remaining fully compatible with WDPA when such information is absent.\nThe core principle relies on default completion combined with explicit and deterministic precedence rules. In the absence of alternative information, WDPA attributes and geometry define a single state whose validity period starts from STATUS_YR, interpreted using a documented date convention, and extends to the reference date of the WDPA release. When earlier legal decisions, previous boundaries, or internal zoning arrangements are identified through supplementary sources, the timeline is split into multiple non overlapping validity intervals. Earlier states inherit WDPA attributes by default, except where legal texts, dated perimeters, or zoning documents explicitly indicate differences, such as temporary protection status, alternative boundaries, changes in management, or a specific internal organization.\nLegal history, boundary evolution, and the establishment of internal zoning are treated as independent sources of temporal segmentation. Dates derived from legal texts, zoning instruments, or boundary modifications jointly structure the timeline. Each resulting interval is then populated with the most specific information available for the period considered, both at the protected area level and, when relevant, for its internal zones. External boundaries are treated as a specific zone type, allowing internal zoning and outer limits to follow a unified spatial logic.\nConcretely, the dataset does not replace WDPA. It is constructed from four complementary components, each with a clearly defined role:\n\nWDPA The global WDPA polygon dataset, used as the baseline reference describing the current state of protected areas.\nSAT - Spatial Amendment Table A table of spatial amendments containing dated historical boundaries and internal zoning geometries that differ from the current WDPA representation.\nFAT — Feature Amendment Table A table of attribute amendments containing dated legal and administrative modifications, such as changes in protection status, management delegation, designation, or governance arrangements.\n\n• CAR - Consolidation and Auditing Rules A stable and transparent set of consolidation rules, expressed as human readable and reproducible code, that specify how WDPA, SAT, and FAT are combined, how temporal segmentation is performed, and how precedence between sources is resolved and made auditable.\nTogether, these components generate a derived dynamic state table in which each WDPA protected area is represented by a sequence of time bounded states. WDPA provides the default current state, while SAT and FAT only intervene when explicit dated information is available, triggering a segmentation of the timeline and, when relevant, a modification of geometry, legal status, or internal zoning as specified by CAR. In the absence of such information, WDPA data are simply extended backward or forward unchanged according to documented conventions.\nThe resulting dataset remains fully compatible with WDPA identifiers and structure, while making temporal assumptions explicit, reproducible, and testable. The contribution therefore lies both in the publication of curated supplementary data on legal and spatial histories (SAT and FAT), and in the transparent consolidation and auditing procedures (CAR) that allow WDPA to be transformed into a time explicit conservation dataset suitable for longitudinal and impact analyses.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Technical proposal (English)</span>"
    ]
  },
  {
    "objectID": "10_references.html",
    "href": "10_references.html",
    "title": "Bibliographie",
    "section": "",
    "text": "Goodman, Steven M., Marie Jeanne Raherilalao, Sébastien Wohlhauser, Jean\nClarck N. Rabenandrasana, Herivololona M. Rakotondratsimba, Fanja\nAndriamialisoa, and Malalarisoa Razafimpahanana. 2018. Les Aires\nProtégées Terrestres de Madagascar: Leur Histoire, Description Et\nBiote. Association Vahatra.\n\n\nVieilledent, Ghislain, Marie Nourtier, Clovis Grinand, Miguel Pedrono,\nAlison Clausen, Tsiky Rabetrano, Jean-Roger Rakotoarijaona, et al. 2020.\n“It’s Not Just Poverty: Unregulated Global Market and Bad\nGovernance Explain Unceasing Deforestation in Western\nMadagascar.” BioRxiv, 2020–07.",
    "crumbs": [
      "Bibliographie"
    ]
  }
]